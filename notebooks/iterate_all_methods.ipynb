{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import git\n",
    "from pm4py.objects.log.importer.xes import factory as xes_import_factory\n",
    "from replearn.eventlog import EventLog\n",
    "from log_iteration import Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC71AA7F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC71AA7F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 24ms/step - loss: 0.1890\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 2s 25ms/step - loss: 0.0124\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.0056\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 2s 22ms/step - loss: 0.0052\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 2s 22ms/step - loss: 0.0050\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.0048\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.0044\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 0.0038\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.0034\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.0031\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72D9B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72D9B708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 4s 8ms/step - loss: 20.0177 - dense_95_loss: 1.8993 - dense_96_loss: 2.2990 - dense_97_loss: 1.8792 - dense_98_loss: 1.9104 - dense_99_loss: 2.2891 - dense_100_loss: 2.2943 - dense_101_loss: 1.9025 - dense_102_loss: 1.9042 - dense_103_loss: 1.8675 - dense_104_loss: 1.7721 - dense_95_accuracy: 0.2406 - dense_96_accuracy: 0.1552 - dense_97_accuracy: 0.2548 - dense_98_accuracy: 0.2282 - dense_99_accuracy: 0.1676 - dense_100_accuracy: 0.1532 - dense_101_accuracy: 0.2794 - dense_102_accuracy: 0.2788 - dense_103_accuracy: 0.3014 - dense_104_accuracy: 0.3178\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 14.8138 - dense_95_loss: 1.4003 - dense_96_loss: 1.6494 - dense_97_loss: 1.4113 - dense_98_loss: 1.5187 - dense_99_loss: 1.6500 - dense_100_loss: 1.6387 - dense_101_loss: 1.4282 - dense_102_loss: 1.3669 - dense_103_loss: 1.3851 - dense_104_loss: 1.3651 - dense_95_accuracy: 0.3452 - dense_96_accuracy: 0.2328 - dense_97_accuracy: 0.3534 - dense_98_accuracy: 0.2734 - dense_99_accuracy: 0.2424 - dense_100_accuracy: 0.2378 - dense_101_accuracy: 0.3624 - dense_102_accuracy: 0.3664 - dense_103_accuracy: 0.3606 - dense_104_accuracy: 0.3670\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 14.0475 - dense_95_loss: 1.3375 - dense_96_loss: 1.5609 - dense_97_loss: 1.3497 - dense_98_loss: 1.4572 - dense_99_loss: 1.5671 - dense_100_loss: 1.5496 - dense_101_loss: 1.3508 - dense_102_loss: 1.2766 - dense_103_loss: 1.2972 - dense_104_loss: 1.3009 - dense_95_accuracy: 0.3540 - dense_96_accuracy: 0.2526 - dense_97_accuracy: 0.3624 - dense_98_accuracy: 0.2892 - dense_99_accuracy: 0.2474 - dense_100_accuracy: 0.2472 - dense_101_accuracy: 0.3604 - dense_102_accuracy: 0.3802 - dense_103_accuracy: 0.3548 - dense_104_accuracy: 0.3668\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 13.9493 - dense_95_loss: 1.3242 - dense_96_loss: 1.5443 - dense_97_loss: 1.3398 - dense_98_loss: 1.4498 - dense_99_loss: 1.5541 - dense_100_loss: 1.5389 - dense_101_loss: 1.3442 - dense_102_loss: 1.2619 - dense_103_loss: 1.3009 - dense_104_loss: 1.2912 - dense_95_accuracy: 0.3560 - dense_96_accuracy: 0.2496 - dense_97_accuracy: 0.3640 - dense_98_accuracy: 0.2920 - dense_99_accuracy: 0.2454 - dense_100_accuracy: 0.2472 - dense_101_accuracy: 0.3598 - dense_102_accuracy: 0.3820 - dense_103_accuracy: 0.3664 - dense_104_accuracy: 0.3712\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 13.7308 - dense_95_loss: 1.3168 - dense_96_loss: 1.5197 - dense_97_loss: 1.3239 - dense_98_loss: 1.4335 - dense_99_loss: 1.5301 - dense_100_loss: 1.5142 - dense_101_loss: 1.3147 - dense_102_loss: 1.2441 - dense_103_loss: 1.2648 - dense_104_loss: 1.2691 - dense_95_accuracy: 0.3544 - dense_96_accuracy: 0.2400 - dense_97_accuracy: 0.3660 - dense_98_accuracy: 0.2790 - dense_99_accuracy: 0.2440 - dense_100_accuracy: 0.2344 - dense_101_accuracy: 0.3702 - dense_102_accuracy: 0.3812 - dense_103_accuracy: 0.3672 - dense_104_accuracy: 0.3700\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 13.6404 - dense_95_loss: 1.3068 - dense_96_loss: 1.5131 - dense_97_loss: 1.3134 - dense_98_loss: 1.4287 - dense_99_loss: 1.5195 - dense_100_loss: 1.5046 - dense_101_loss: 1.3048 - dense_102_loss: 1.2335 - dense_103_loss: 1.2552 - dense_104_loss: 1.2608 - dense_95_accuracy: 0.3608 - dense_96_accuracy: 0.2466 - dense_97_accuracy: 0.3634 - dense_98_accuracy: 0.2888 - dense_99_accuracy: 0.2532 - dense_100_accuracy: 0.2488 - dense_101_accuracy: 0.3708 - dense_102_accuracy: 0.3840 - dense_103_accuracy: 0.3722 - dense_104_accuracy: 0.3726\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 14ms/step - loss: 13.5999 - dense_95_loss: 1.3066 - dense_96_loss: 1.5111 - dense_97_loss: 1.3044 - dense_98_loss: 1.4168 - dense_99_loss: 1.5144 - dense_100_loss: 1.4971 - dense_101_loss: 1.3059 - dense_102_loss: 1.2307 - dense_103_loss: 1.2561 - dense_104_loss: 1.2569 - dense_95_accuracy: 0.3664 - dense_96_accuracy: 0.2496 - dense_97_accuracy: 0.3752 - dense_98_accuracy: 0.2950 - dense_99_accuracy: 0.2532 - dense_100_accuracy: 0.2538 - dense_101_accuracy: 0.3682 - dense_102_accuracy: 0.3886 - dense_103_accuracy: 0.3672 - dense_104_accuracy: 0.3794\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 13ms/step - loss: 13.5802 - dense_95_loss: 1.3042 - dense_96_loss: 1.5059 - dense_97_loss: 1.3070 - dense_98_loss: 1.4221 - dense_99_loss: 1.5099 - dense_100_loss: 1.4946 - dense_101_loss: 1.3011 - dense_102_loss: 1.2318 - dense_103_loss: 1.2508 - dense_104_loss: 1.2529 - dense_95_accuracy: 0.3674 - dense_96_accuracy: 0.2520 - dense_97_accuracy: 0.3752 - dense_98_accuracy: 0.2902 - dense_99_accuracy: 0.2502 - dense_100_accuracy: 0.2526 - dense_101_accuracy: 0.3684 - dense_102_accuracy: 0.3796 - dense_103_accuracy: 0.3714 - dense_104_accuracy: 0.3800\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 15ms/step - loss: 13.6967 - dense_95_loss: 1.3114 - dense_96_loss: 1.5220 - dense_97_loss: 1.3184 - dense_98_loss: 1.4281 - dense_99_loss: 1.5238 - dense_100_loss: 1.5112 - dense_101_loss: 1.3121 - dense_102_loss: 1.2390 - dense_103_loss: 1.2616 - dense_104_loss: 1.2691 - dense_95_accuracy: 0.3618 - dense_96_accuracy: 0.2480 - dense_97_accuracy: 0.3666 - dense_98_accuracy: 0.2930 - dense_99_accuracy: 0.2470 - dense_100_accuracy: 0.2432 - dense_101_accuracy: 0.3706 - dense_102_accuracy: 0.3870 - dense_103_accuracy: 0.3754 - dense_104_accuracy: 0.3730\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 13ms/step - loss: 13.6257 - dense_95_loss: 1.3058 - dense_96_loss: 1.5093 - dense_97_loss: 1.3114 - dense_98_loss: 1.4175 - dense_99_loss: 1.5119 - dense_100_loss: 1.5010 - dense_101_loss: 1.3132 - dense_102_loss: 1.2365 - dense_103_loss: 1.2622 - dense_104_loss: 1.2569 - dense_95_accuracy: 0.3674 - dense_96_accuracy: 0.2540 - dense_97_accuracy: 0.3744 - dense_98_accuracy: 0.3028 - dense_99_accuracy: 0.2632 - dense_100_accuracy: 0.2534 - dense_101_accuracy: 0.3618 - dense_102_accuracy: 0.3810 - dense_103_accuracy: 0.3670 - dense_104_accuracy: 0.3776\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC0027B168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC0027B168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC001CDE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC001CDE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 5s 10ms/step - loss: 19.1487 - dense_105_loss: 1.8060 - dense_106_loss: 2.1911 - dense_107_loss: 1.7844 - dense_108_loss: 1.8342 - dense_109_loss: 2.1902 - dense_110_loss: 2.1827 - dense_111_loss: 1.8376 - dense_112_loss: 1.7964 - dense_113_loss: 1.8032 - dense_114_loss: 1.7232 - dense_105_accuracy: 0.2648 - dense_106_accuracy: 0.1722 - dense_107_accuracy: 0.2854 - dense_108_accuracy: 0.2322 - dense_109_accuracy: 0.1838 - dense_110_accuracy: 0.1728 - dense_111_accuracy: 0.2872 - dense_112_accuracy: 0.2910 - dense_113_accuracy: 0.3062 - dense_114_accuracy: 0.2928\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 14.6235 - dense_105_loss: 1.3928 - dense_106_loss: 1.6317 - dense_107_loss: 1.3924 - dense_108_loss: 1.5063 - dense_109_loss: 1.6345 - dense_110_loss: 1.6225 - dense_111_loss: 1.4003 - dense_112_loss: 1.3357 - dense_113_loss: 1.3604 - dense_114_loss: 1.3470 - dense_105_accuracy: 0.3480 - dense_106_accuracy: 0.2456 - dense_107_accuracy: 0.3510 - dense_108_accuracy: 0.2836 - dense_109_accuracy: 0.2468 - dense_110_accuracy: 0.2480 - dense_111_accuracy: 0.3546 - dense_112_accuracy: 0.3706 - dense_113_accuracy: 0.3646 - dense_114_accuracy: 0.3612\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 13ms/step - loss: 14.1702 - dense_105_loss: 1.3504 - dense_106_loss: 1.5737 - dense_107_loss: 1.3639 - dense_108_loss: 1.4693 - dense_109_loss: 1.5843 - dense_110_loss: 1.5647 - dense_111_loss: 1.3528 - dense_112_loss: 1.2884 - dense_113_loss: 1.3147 - dense_114_loss: 1.3080 - dense_105_accuracy: 0.3516 - dense_106_accuracy: 0.2480 - dense_107_accuracy: 0.3614 - dense_108_accuracy: 0.2798 - dense_109_accuracy: 0.2486 - dense_110_accuracy: 0.2456 - dense_111_accuracy: 0.3624 - dense_112_accuracy: 0.3764 - dense_113_accuracy: 0.3592 - dense_114_accuracy: 0.3654\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 13.7383 - dense_105_loss: 1.3095 - dense_106_loss: 1.5238 - dense_107_loss: 1.3231 - dense_108_loss: 1.4311 - dense_109_loss: 1.5320 - dense_110_loss: 1.5156 - dense_111_loss: 1.3161 - dense_112_loss: 1.2454 - dense_113_loss: 1.2728 - dense_114_loss: 1.2688 - dense_105_accuracy: 0.3544 - dense_106_accuracy: 0.2600 - dense_107_accuracy: 0.3706 - dense_108_accuracy: 0.2912 - dense_109_accuracy: 0.2598 - dense_110_accuracy: 0.2554 - dense_111_accuracy: 0.3762 - dense_112_accuracy: 0.3886 - dense_113_accuracy: 0.3714 - dense_114_accuracy: 0.3772\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 13.6813 - dense_105_loss: 1.3083 - dense_106_loss: 1.5153 - dense_107_loss: 1.3186 - dense_108_loss: 1.4299 - dense_109_loss: 1.5229 - dense_110_loss: 1.5079 - dense_111_loss: 1.3109 - dense_112_loss: 1.2369 - dense_113_loss: 1.2657 - dense_114_loss: 1.2648 - dense_105_accuracy: 0.3654 - dense_106_accuracy: 0.2512 - dense_107_accuracy: 0.3756 - dense_108_accuracy: 0.2852 - dense_109_accuracy: 0.2468 - dense_110_accuracy: 0.2472 - dense_111_accuracy: 0.3656 - dense_112_accuracy: 0.3828 - dense_113_accuracy: 0.3644 - dense_114_accuracy: 0.3718\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 13.6094 - dense_105_loss: 1.3043 - dense_106_loss: 1.5068 - dense_107_loss: 1.3142 - dense_108_loss: 1.4225 - dense_109_loss: 1.5148 - dense_110_loss: 1.4990 - dense_111_loss: 1.3067 - dense_112_loss: 1.2298 - dense_113_loss: 1.2563 - dense_114_loss: 1.2550 - dense_105_accuracy: 0.3568 - dense_106_accuracy: 0.2428 - dense_107_accuracy: 0.3676 - dense_108_accuracy: 0.2842 - dense_109_accuracy: 0.2502 - dense_110_accuracy: 0.2412 - dense_111_accuracy: 0.3676 - dense_112_accuracy: 0.3766 - dense_113_accuracy: 0.3628 - dense_114_accuracy: 0.3682\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 13.6102 - dense_105_loss: 1.3053 - dense_106_loss: 1.5068 - dense_107_loss: 1.3088 - dense_108_loss: 1.4206 - dense_109_loss: 1.5166 - dense_110_loss: 1.4997 - dense_111_loss: 1.3060 - dense_112_loss: 1.2307 - dense_113_loss: 1.2568 - dense_114_loss: 1.2589 - dense_105_accuracy: 0.3620 - dense_106_accuracy: 0.2494 - dense_107_accuracy: 0.3728 - dense_108_accuracy: 0.2918 - dense_109_accuracy: 0.2442 - dense_110_accuracy: 0.2428 - dense_111_accuracy: 0.3730 - dense_112_accuracy: 0.3810 - dense_113_accuracy: 0.3672 - dense_114_accuracy: 0.3688\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 13.5292 - dense_105_loss: 1.2994 - dense_106_loss: 1.4974 - dense_107_loss: 1.3031 - dense_108_loss: 1.4168 - dense_109_loss: 1.5069 - dense_110_loss: 1.4904 - dense_111_loss: 1.2933 - dense_112_loss: 1.2201 - dense_113_loss: 1.2527 - dense_114_loss: 1.2492 - dense_105_accuracy: 0.3638 - dense_106_accuracy: 0.2476 - dense_107_accuracy: 0.3680 - dense_108_accuracy: 0.2872 - dense_109_accuracy: 0.2424 - dense_110_accuracy: 0.2492 - dense_111_accuracy: 0.3670 - dense_112_accuracy: 0.3802 - dense_113_accuracy: 0.3606 - dense_114_accuracy: 0.3724\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 13.5516 - dense_105_loss: 1.3008 - dense_106_loss: 1.5004 - dense_107_loss: 1.3066 - dense_108_loss: 1.4189 - dense_109_loss: 1.5079 - dense_110_loss: 1.4922 - dense_111_loss: 1.2949 - dense_112_loss: 1.2227 - dense_113_loss: 1.2527 - dense_114_loss: 1.2545 - dense_105_accuracy: 0.3626 - dense_106_accuracy: 0.2550 - dense_107_accuracy: 0.3724 - dense_108_accuracy: 0.2890 - dense_109_accuracy: 0.2514 - dense_110_accuracy: 0.2514 - dense_111_accuracy: 0.3682 - dense_112_accuracy: 0.3816 - dense_113_accuracy: 0.3678 - dense_114_accuracy: 0.3762\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 13.5057 - dense_105_loss: 1.2982 - dense_106_loss: 1.4945 - dense_107_loss: 1.2996 - dense_108_loss: 1.4142 - dense_109_loss: 1.5027 - dense_110_loss: 1.4867 - dense_111_loss: 1.2938 - dense_112_loss: 1.2201 - dense_113_loss: 1.2467 - dense_114_loss: 1.2491 - dense_105_accuracy: 0.3704 - dense_106_accuracy: 0.2478 - dense_107_accuracy: 0.3778 - dense_108_accuracy: 0.2842 - dense_109_accuracy: 0.2484 - dense_110_accuracy: 0.2490 - dense_111_accuracy: 0.3720 - dense_112_accuracy: 0.3768 - dense_113_accuracy: 0.3704 - dense_114_accuracy: 0.3790\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC002498B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC002498B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00B49558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00B49558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 14ms/step - loss: 0.2484\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.2429\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2313\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.2096\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.1715\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.1217\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0693\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0358\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0204\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0139\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC01E5F828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC01E5F828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "16/16 [==============================] - 4s 8ms/step - loss: 23.6491 - dense_118_loss: 2.0933 - dense_119_loss: 2.7322 - dense_120_loss: 2.2746 - dense_121_loss: 2.2422 - dense_122_loss: 2.7622 - dense_123_loss: 2.7911 - dense_124_loss: 2.3602 - dense_125_loss: 2.3213 - dense_126_loss: 2.1595 - dense_127_loss: 1.9126 - dense_118_accuracy: 0.2380 - dense_119_accuracy: 0.0990 - dense_120_accuracy: 0.1820 - dense_121_accuracy: 0.1260 - dense_122_accuracy: 0.1110 - dense_123_accuracy: 0.0980 - dense_124_accuracy: 0.1780 - dense_125_accuracy: 0.1670 - dense_126_accuracy: 0.3240 - dense_127_accuracy: 0.3750 \n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.5364 - dense_118_loss: 1.7251 - dense_119_loss: 2.3510 - dense_120_loss: 2.0193 - dense_121_loss: 2.1076 - dense_122_loss: 2.3772 - dense_123_loss: 2.4021 - dense_124_loss: 2.0572 - dense_125_loss: 2.1192 - dense_126_loss: 1.8295 - dense_127_loss: 1.5483 - dense_118_accuracy: 0.2990 - dense_119_accuracy: 0.1330 - dense_120_accuracy: 0.2750 - dense_121_accuracy: 0.1620 - dense_122_accuracy: 0.1440 - dense_123_accuracy: 0.1380 - dense_124_accuracy: 0.2380 - dense_125_accuracy: 0.1640 - dense_126_accuracy: 0.3590 - dense_127_accuracy: 0.4350\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.7981 - dense_118_loss: 1.5953 - dense_119_loss: 1.8568 - dense_120_loss: 1.6063 - dense_121_loss: 1.7631 - dense_122_loss: 1.8949 - dense_123_loss: 1.9175 - dense_124_loss: 1.7307 - dense_125_loss: 1.8249 - dense_126_loss: 1.2909 - dense_127_loss: 1.3177 - dense_118_accuracy: 0.3510 - dense_119_accuracy: 0.2160 - dense_120_accuracy: 0.3520 - dense_121_accuracy: 0.2380 - dense_122_accuracy: 0.2270 - dense_123_accuracy: 0.2090 - dense_124_accuracy: 0.2590 - dense_125_accuracy: 0.2340 - dense_126_accuracy: 0.4900 - dense_127_accuracy: 0.5220\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.6559 - dense_118_loss: 1.5073 - dense_119_loss: 1.7401 - dense_120_loss: 1.4984 - dense_121_loss: 1.6313 - dense_122_loss: 1.7770 - dense_123_loss: 1.7965 - dense_124_loss: 1.6016 - dense_125_loss: 1.6568 - dense_126_loss: 1.2140 - dense_127_loss: 1.2328 - dense_118_accuracy: 0.3520 - dense_119_accuracy: 0.2140 - dense_120_accuracy: 0.3890 - dense_121_accuracy: 0.2320 - dense_122_accuracy: 0.2250 - dense_123_accuracy: 0.2400 - dense_124_accuracy: 0.2710 - dense_125_accuracy: 0.2300 - dense_126_accuracy: 0.4980 - dense_127_accuracy: 0.5120\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 15.5652 - dense_118_loss: 1.4895 - dense_119_loss: 1.7227 - dense_120_loss: 1.4926 - dense_121_loss: 1.6297 - dense_122_loss: 1.7667 - dense_123_loss: 1.7864 - dense_124_loss: 1.5906 - dense_125_loss: 1.6482 - dense_126_loss: 1.2049 - dense_127_loss: 1.2340 - dense_118_accuracy: 0.3540 - dense_119_accuracy: 0.2260 - dense_120_accuracy: 0.3700 - dense_121_accuracy: 0.2320 - dense_122_accuracy: 0.2410 - dense_123_accuracy: 0.2250 - dense_124_accuracy: 0.2760 - dense_125_accuracy: 0.2250 - dense_126_accuracy: 0.5010 - dense_127_accuracy: 0.5240\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.3783 - dense_118_loss: 1.4605 - dense_119_loss: 1.6934 - dense_120_loss: 1.4812 - dense_121_loss: 1.6182 - dense_122_loss: 1.7555 - dense_123_loss: 1.7663 - dense_124_loss: 1.5713 - dense_125_loss: 1.6324 - dense_126_loss: 1.1938 - dense_127_loss: 1.2056 - dense_118_accuracy: 0.3550 - dense_119_accuracy: 0.2310 - dense_120_accuracy: 0.3730 - dense_121_accuracy: 0.2310 - dense_122_accuracy: 0.1960 - dense_123_accuracy: 0.2190 - dense_124_accuracy: 0.2640 - dense_125_accuracy: 0.2220 - dense_126_accuracy: 0.5120 - dense_127_accuracy: 0.5290\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.1179 - dense_118_loss: 1.4262 - dense_119_loss: 1.6502 - dense_120_loss: 1.4555 - dense_121_loss: 1.6144 - dense_122_loss: 1.7151 - dense_123_loss: 1.7391 - dense_124_loss: 1.5495 - dense_125_loss: 1.5956 - dense_126_loss: 1.1830 - dense_127_loss: 1.1894 - dense_118_accuracy: 0.3780 - dense_119_accuracy: 0.2560 - dense_120_accuracy: 0.3910 - dense_121_accuracy: 0.2480 - dense_122_accuracy: 0.2590 - dense_123_accuracy: 0.2380 - dense_124_accuracy: 0.2960 - dense_125_accuracy: 0.2440 - dense_126_accuracy: 0.5060 - dense_127_accuracy: 0.5270\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.8324 - dense_118_loss: 1.4102 - dense_119_loss: 1.6199 - dense_120_loss: 1.4208 - dense_121_loss: 1.5845 - dense_122_loss: 1.6763 - dense_123_loss: 1.6985 - dense_124_loss: 1.5248 - dense_125_loss: 1.5751 - dense_126_loss: 1.1552 - dense_127_loss: 1.1672 - dense_118_accuracy: 0.3690 - dense_119_accuracy: 0.2630 - dense_120_accuracy: 0.3990 - dense_121_accuracy: 0.2650 - dense_122_accuracy: 0.2650 - dense_123_accuracy: 0.2510 - dense_124_accuracy: 0.2970 - dense_125_accuracy: 0.2680 - dense_126_accuracy: 0.5230 - dense_127_accuracy: 0.5460\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.5083 - dense_118_loss: 1.3898 - dense_119_loss: 1.5824 - dense_120_loss: 1.4028 - dense_121_loss: 1.5427 - dense_122_loss: 1.6489 - dense_123_loss: 1.6585 - dense_124_loss: 1.4881 - dense_125_loss: 1.5511 - dense_126_loss: 1.1223 - dense_127_loss: 1.1216 - dense_118_accuracy: 0.3800 - dense_119_accuracy: 0.2590 - dense_120_accuracy: 0.3900 - dense_121_accuracy: 0.2760 - dense_122_accuracy: 0.2470 - dense_123_accuracy: 0.2640 - dense_124_accuracy: 0.3090 - dense_125_accuracy: 0.2760 - dense_126_accuracy: 0.5230 - dense_127_accuracy: 0.5460\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 14.3499 - dense_118_loss: 1.3777 - dense_119_loss: 1.5639 - dense_120_loss: 1.3914 - dense_121_loss: 1.5265 - dense_122_loss: 1.6291 - dense_123_loss: 1.6360 - dense_124_loss: 1.4737 - dense_125_loss: 1.5357 - dense_126_loss: 1.1094 - dense_127_loss: 1.1064 - dense_118_accuracy: 0.3970 - dense_119_accuracy: 0.2710 - dense_120_accuracy: 0.3970 - dense_121_accuracy: 0.3030 - dense_122_accuracy: 0.2590 - dense_123_accuracy: 0.2900 - dense_124_accuracy: 0.3220 - dense_125_accuracy: 0.2690 - dense_126_accuracy: 0.5180 - dense_127_accuracy: 0.5700\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC7529DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC7529DDC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC01416708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC01416708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "16/16 [==============================] - 4s 9ms/step - loss: 23.0524 - dense_128_loss: 2.0229 - dense_129_loss: 2.6900 - dense_130_loss: 2.2173 - dense_131_loss: 2.2178 - dense_132_loss: 2.7108 - dense_133_loss: 2.7469 - dense_134_loss: 2.2594 - dense_135_loss: 2.3279 - dense_136_loss: 2.0441 - dense_137_loss: 1.8152 - dense_128_accuracy: 0.2500 - dense_129_accuracy: 0.1270 - dense_130_accuracy: 0.2530 - dense_131_accuracy: 0.1180 - dense_132_accuracy: 0.1460 - dense_133_accuracy: 0.1230 - dense_134_accuracy: 0.1810 - dense_135_accuracy: 0.1560 - dense_136_accuracy: 0.3430 - dense_137_accuracy: 0.4080\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 17.1912 - dense_128_loss: 1.6345 - dense_129_loss: 1.9294 - dense_130_loss: 1.6352 - dense_131_loss: 1.7996 - dense_132_loss: 1.9563 - dense_133_loss: 2.0341 - dense_134_loss: 1.7585 - dense_135_loss: 1.8227 - dense_136_loss: 1.3236 - dense_137_loss: 1.2972 - dense_128_accuracy: 0.3430 - dense_129_accuracy: 0.2290 - dense_130_accuracy: 0.3460 - dense_131_accuracy: 0.1950 - dense_132_accuracy: 0.2100 - dense_133_accuracy: 0.2050 - dense_134_accuracy: 0.2280 - dense_135_accuracy: 0.2220 - dense_136_accuracy: 0.4910 - dense_137_accuracy: 0.5020\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.7146 - dense_128_loss: 1.5088 - dense_129_loss: 1.7287 - dense_130_loss: 1.5100 - dense_131_loss: 1.6555 - dense_132_loss: 1.7736 - dense_133_loss: 1.8117 - dense_134_loss: 1.5908 - dense_135_loss: 1.6773 - dense_136_loss: 1.2210 - dense_137_loss: 1.2372 - dense_128_accuracy: 0.3470 - dense_129_accuracy: 0.2120 - dense_130_accuracy: 0.3640 - dense_131_accuracy: 0.2230 - dense_132_accuracy: 0.2040 - dense_133_accuracy: 0.2080 - dense_134_accuracy: 0.2620 - dense_135_accuracy: 0.2010 - dense_136_accuracy: 0.4910 - dense_137_accuracy: 0.5190\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.1866 - dense_128_loss: 1.4451 - dense_129_loss: 1.6575 - dense_130_loss: 1.4552 - dense_131_loss: 1.6329 - dense_132_loss: 1.7199 - dense_133_loss: 1.7275 - dense_134_loss: 1.5496 - dense_135_loss: 1.6119 - dense_136_loss: 1.1864 - dense_137_loss: 1.2005 - dense_128_accuracy: 0.3670 - dense_129_accuracy: 0.2190 - dense_130_accuracy: 0.3750 - dense_131_accuracy: 0.2190 - dense_132_accuracy: 0.2200 - dense_133_accuracy: 0.2360 - dense_134_accuracy: 0.2720 - dense_135_accuracy: 0.2410 - dense_136_accuracy: 0.5110 - dense_137_accuracy: 0.5350\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.9138 - dense_128_loss: 1.4239 - dense_129_loss: 1.6257 - dense_130_loss: 1.4358 - dense_131_loss: 1.6105 - dense_132_loss: 1.6835 - dense_133_loss: 1.6932 - dense_134_loss: 1.5292 - dense_135_loss: 1.5852 - dense_136_loss: 1.1556 - dense_137_loss: 1.1712 - dense_128_accuracy: 0.3660 - dense_129_accuracy: 0.2270 - dense_130_accuracy: 0.3760 - dense_131_accuracy: 0.2460 - dense_132_accuracy: 0.2530 - dense_133_accuracy: 0.2480 - dense_134_accuracy: 0.2930 - dense_135_accuracy: 0.2390 - dense_136_accuracy: 0.5240 - dense_137_accuracy: 0.5430\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 14.7570 - dense_128_loss: 1.4130 - dense_129_loss: 1.6082 - dense_130_loss: 1.4219 - dense_131_loss: 1.5916 - dense_132_loss: 1.6699 - dense_133_loss: 1.6772 - dense_134_loss: 1.5075 - dense_135_loss: 1.5770 - dense_136_loss: 1.1455 - dense_137_loss: 1.1451 - dense_128_accuracy: 0.3750 - dense_129_accuracy: 0.2290 - dense_130_accuracy: 0.3870 - dense_131_accuracy: 0.2280 - dense_132_accuracy: 0.2530 - dense_133_accuracy: 0.2370 - dense_134_accuracy: 0.2810 - dense_135_accuracy: 0.2300 - dense_136_accuracy: 0.4860 - dense_137_accuracy: 0.5420\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 14.6765 - dense_128_loss: 1.3993 - dense_129_loss: 1.6032 - dense_130_loss: 1.4234 - dense_131_loss: 1.5731 - dense_132_loss: 1.6645 - dense_133_loss: 1.6725 - dense_134_loss: 1.5080 - dense_135_loss: 1.5686 - dense_136_loss: 1.1279 - dense_137_loss: 1.1361 - dense_128_accuracy: 0.3650 - dense_129_accuracy: 0.2470 - dense_130_accuracy: 0.3720 - dense_131_accuracy: 0.2390 - dense_132_accuracy: 0.2490 - dense_133_accuracy: 0.2440 - dense_134_accuracy: 0.2880 - dense_135_accuracy: 0.2470 - dense_136_accuracy: 0.5200 - dense_137_accuracy: 0.5390\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 14.6049 - dense_128_loss: 1.3951 - dense_129_loss: 1.5959 - dense_130_loss: 1.4098 - dense_131_loss: 1.5572 - dense_132_loss: 1.6557 - dense_133_loss: 1.6634 - dense_134_loss: 1.5076 - dense_135_loss: 1.5696 - dense_136_loss: 1.1233 - dense_137_loss: 1.1274 - dense_128_accuracy: 0.3460 - dense_129_accuracy: 0.2470 - dense_130_accuracy: 0.3810 - dense_131_accuracy: 0.2420 - dense_132_accuracy: 0.2390 - dense_133_accuracy: 0.2340 - dense_134_accuracy: 0.2970 - dense_135_accuracy: 0.2370 - dense_136_accuracy: 0.5000 - dense_137_accuracy: 0.5410\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 14.6038 - dense_128_loss: 1.4057 - dense_129_loss: 1.5988 - dense_130_loss: 1.4044 - dense_131_loss: 1.5519 - dense_132_loss: 1.6534 - dense_133_loss: 1.6716 - dense_134_loss: 1.4962 - dense_135_loss: 1.5696 - dense_136_loss: 1.1309 - dense_137_loss: 1.1213 - dense_128_accuracy: 0.3700 - dense_129_accuracy: 0.2460 - dense_130_accuracy: 0.3800 - dense_131_accuracy: 0.2460 - dense_132_accuracy: 0.2670 - dense_133_accuracy: 0.2430 - dense_134_accuracy: 0.2960 - dense_135_accuracy: 0.2490 - dense_136_accuracy: 0.5060 - dense_137_accuracy: 0.5360\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 14.5818 - dense_128_loss: 1.4037 - dense_129_loss: 1.5927 - dense_130_loss: 1.4035 - dense_131_loss: 1.5584 - dense_132_loss: 1.6574 - dense_133_loss: 1.6678 - dense_134_loss: 1.4957 - dense_135_loss: 1.5715 - dense_136_loss: 1.1162 - dense_137_loss: 1.1151 - dense_128_accuracy: 0.3560 - dense_129_accuracy: 0.2390 - dense_130_accuracy: 0.3700 - dense_131_accuracy: 0.2510 - dense_132_accuracy: 0.2440 - dense_133_accuracy: 0.2510 - dense_134_accuracy: 0.2890 - dense_135_accuracy: 0.2440 - dense_136_accuracy: 0.5050 - dense_137_accuracy: 0.5360\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC768C0678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC768C0678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC73453D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC73453D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 13ms/step - loss: 0.2487\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.2431\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2312\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2083\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1682\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1134\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0628\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0319\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0179\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0125\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC735FBE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC735FBE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "16/16 [==============================] - 4s 9ms/step - loss: 23.8431 - dense_141_loss: 2.0609 - dense_142_loss: 2.7579 - dense_143_loss: 2.2797 - dense_144_loss: 2.3124 - dense_145_loss: 2.8039 - dense_146_loss: 2.8329 - dense_147_loss: 2.3548 - dense_148_loss: 2.3674 - dense_149_loss: 2.1500 - dense_150_loss: 1.9231 - dense_141_accuracy: 0.1760 - dense_142_accuracy: 0.0780 - dense_143_accuracy: 0.1810 - dense_144_accuracy: 0.1210 - dense_145_accuracy: 0.0970 - dense_146_accuracy: 0.0900 - dense_147_accuracy: 0.1790 - dense_148_accuracy: 0.1790 - dense_149_accuracy: 0.3350 - dense_150_accuracy: 0.3850\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 20.5833 - dense_141_loss: 1.6691 - dense_142_loss: 2.4061 - dense_143_loss: 1.9628 - dense_144_loss: 2.1586 - dense_145_loss: 2.3428 - dense_146_loss: 2.4575 - dense_147_loss: 2.1367 - dense_148_loss: 2.1478 - dense_149_loss: 1.7381 - dense_150_loss: 1.5638 - dense_141_accuracy: 0.3240 - dense_142_accuracy: 0.1370 - dense_143_accuracy: 0.2740 - dense_144_accuracy: 0.1300 - dense_145_accuracy: 0.1660 - dense_146_accuracy: 0.1400 - dense_147_accuracy: 0.2240 - dense_148_accuracy: 0.1960 - dense_149_accuracy: 0.3730 - dense_150_accuracy: 0.4340\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 16.9213 - dense_141_loss: 1.5538 - dense_142_loss: 1.9209 - dense_143_loss: 1.5941 - dense_144_loss: 1.7633 - dense_145_loss: 1.8551 - dense_146_loss: 2.0302 - dense_147_loss: 1.7775 - dense_148_loss: 1.8138 - dense_149_loss: 1.3254 - dense_150_loss: 1.2872 - dense_141_accuracy: 0.3870 - dense_142_accuracy: 0.2190 - dense_143_accuracy: 0.3500 - dense_144_accuracy: 0.2590 - dense_145_accuracy: 0.2320 - dense_146_accuracy: 0.2190 - dense_147_accuracy: 0.2470 - dense_148_accuracy: 0.2540 - dense_149_accuracy: 0.4790 - dense_150_accuracy: 0.4980\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 16.0287 - dense_141_loss: 1.4712 - dense_142_loss: 1.7992 - dense_143_loss: 1.5438 - dense_144_loss: 1.6745 - dense_145_loss: 1.7542 - dense_146_loss: 1.9175 - dense_147_loss: 1.6709 - dense_148_loss: 1.6951 - dense_149_loss: 1.2392 - dense_150_loss: 1.2633 - dense_141_accuracy: 0.3630 - dense_142_accuracy: 0.2240 - dense_143_accuracy: 0.3650 - dense_144_accuracy: 0.2380 - dense_145_accuracy: 0.2300 - dense_146_accuracy: 0.2100 - dense_147_accuracy: 0.2390 - dense_148_accuracy: 0.2360 - dense_149_accuracy: 0.5090 - dense_150_accuracy: 0.5100\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 15.7701 - dense_141_loss: 1.4357 - dense_142_loss: 1.7717 - dense_143_loss: 1.5186 - dense_144_loss: 1.6385 - dense_145_loss: 1.7421 - dense_146_loss: 1.8912 - dense_147_loss: 1.6527 - dense_148_loss: 1.6689 - dense_149_loss: 1.2146 - dense_150_loss: 1.2362 - dense_141_accuracy: 0.3780 - dense_142_accuracy: 0.2270 - dense_143_accuracy: 0.3520 - dense_144_accuracy: 0.2440 - dense_145_accuracy: 0.2150 - dense_146_accuracy: 0.2200 - dense_147_accuracy: 0.2630 - dense_148_accuracy: 0.2500 - dense_149_accuracy: 0.5160 - dense_150_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 15.5035 - dense_141_loss: 1.4013 - dense_142_loss: 1.7386 - dense_143_loss: 1.4958 - dense_144_loss: 1.6317 - dense_145_loss: 1.7133 - dense_146_loss: 1.8623 - dense_147_loss: 1.6234 - dense_148_loss: 1.6345 - dense_149_loss: 1.1890 - dense_150_loss: 1.2134 - dense_141_accuracy: 0.3940 - dense_142_accuracy: 0.2680 - dense_143_accuracy: 0.3570 - dense_144_accuracy: 0.2550 - dense_145_accuracy: 0.2350 - dense_146_accuracy: 0.2220 - dense_147_accuracy: 0.2770 - dense_148_accuracy: 0.2520 - dense_149_accuracy: 0.5010 - dense_150_accuracy: 0.5190\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 15.2725 - dense_141_loss: 1.3796 - dense_142_loss: 1.7016 - dense_143_loss: 1.4815 - dense_144_loss: 1.6281 - dense_145_loss: 1.6832 - dense_146_loss: 1.8325 - dense_147_loss: 1.6004 - dense_148_loss: 1.6101 - dense_149_loss: 1.1692 - dense_150_loss: 1.1862 - dense_141_accuracy: 0.3960 - dense_142_accuracy: 0.2480 - dense_143_accuracy: 0.3440 - dense_144_accuracy: 0.2530 - dense_145_accuracy: 0.2460 - dense_146_accuracy: 0.2520 - dense_147_accuracy: 0.3060 - dense_148_accuracy: 0.2690 - dense_149_accuracy: 0.5080 - dense_150_accuracy: 0.5270\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.9833 - dense_141_loss: 1.3595 - dense_142_loss: 1.6647 - dense_143_loss: 1.4482 - dense_144_loss: 1.6071 - dense_145_loss: 1.6373 - dense_146_loss: 1.8063 - dense_147_loss: 1.5783 - dense_148_loss: 1.5793 - dense_149_loss: 1.1477 - dense_150_loss: 1.1549 - dense_141_accuracy: 0.3950 - dense_142_accuracy: 0.2760 - dense_143_accuracy: 0.3600 - dense_144_accuracy: 0.2470 - dense_145_accuracy: 0.2640 - dense_146_accuracy: 0.2400 - dense_147_accuracy: 0.3100 - dense_148_accuracy: 0.2710 - dense_149_accuracy: 0.5110 - dense_150_accuracy: 0.5330\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 14.7935 - dense_141_loss: 1.3507 - dense_142_loss: 1.6419 - dense_143_loss: 1.4386 - dense_144_loss: 1.5841 - dense_145_loss: 1.6190 - dense_146_loss: 1.7771 - dense_147_loss: 1.5501 - dense_148_loss: 1.5726 - dense_149_loss: 1.1320 - dense_150_loss: 1.1273 - dense_141_accuracy: 0.3970 - dense_142_accuracy: 0.2810 - dense_143_accuracy: 0.3780 - dense_144_accuracy: 0.2770 - dense_145_accuracy: 0.2820 - dense_146_accuracy: 0.2780 - dense_147_accuracy: 0.3230 - dense_148_accuracy: 0.2840 - dense_149_accuracy: 0.5220 - dense_150_accuracy: 0.5390\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 14.6455 - dense_141_loss: 1.3414 - dense_142_loss: 1.6240 - dense_143_loss: 1.4297 - dense_144_loss: 1.5702 - dense_145_loss: 1.5989 - dense_146_loss: 1.7626 - dense_147_loss: 1.5312 - dense_148_loss: 1.5585 - dense_149_loss: 1.1197 - dense_150_loss: 1.1094 - dense_141_accuracy: 0.4150 - dense_142_accuracy: 0.2970 - dense_143_accuracy: 0.3790 - dense_144_accuracy: 0.2820 - dense_145_accuracy: 0.2830 - dense_146_accuracy: 0.2870 - dense_147_accuracy: 0.3440 - dense_148_accuracy: 0.2930 - dense_149_accuracy: 0.5290 - dense_150_accuracy: 0.5500\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC73565AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC73565AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72D9BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72D9BEE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "16/16 [==============================] - 3s 10ms/step - loss: 22.6839 - dense_151_loss: 1.9537 - dense_152_loss: 2.5917 - dense_153_loss: 2.2414 - dense_154_loss: 2.2021 - dense_155_loss: 2.5961 - dense_156_loss: 2.7062 - dense_157_loss: 2.3210 - dense_158_loss: 2.3173 - dense_159_loss: 1.9674 - dense_160_loss: 1.7869 - dense_151_accuracy: 0.2880 - dense_152_accuracy: 0.1590 - dense_153_accuracy: 0.1910 - dense_154_accuracy: 0.1730 - dense_155_accuracy: 0.1380 - dense_156_accuracy: 0.1400 - dense_157_accuracy: 0.1360 - dense_158_accuracy: 0.1240 - dense_159_accuracy: 0.3750 - dense_160_accuracy: 0.4250\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 17.4271 - dense_151_loss: 1.5939 - dense_152_loss: 1.9461 - dense_153_loss: 1.6590 - dense_154_loss: 1.7934 - dense_155_loss: 1.9392 - dense_156_loss: 2.0978 - dense_157_loss: 1.8365 - dense_158_loss: 1.8868 - dense_159_loss: 1.3376 - dense_160_loss: 1.3368 - dense_151_accuracy: 0.3470 - dense_152_accuracy: 0.2100 - dense_153_accuracy: 0.3610 - dense_154_accuracy: 0.2290 - dense_155_accuracy: 0.2010 - dense_156_accuracy: 0.1740 - dense_157_accuracy: 0.2220 - dense_158_accuracy: 0.1980 - dense_159_accuracy: 0.4630 - dense_160_accuracy: 0.5060\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 16.2995 - dense_151_loss: 1.5003 - dense_152_loss: 1.8363 - dense_153_loss: 1.5764 - dense_154_loss: 1.6942 - dense_155_loss: 1.7880 - dense_156_loss: 1.9432 - dense_157_loss: 1.7205 - dense_158_loss: 1.7422 - dense_159_loss: 1.2463 - dense_160_loss: 1.2521 - dense_151_accuracy: 0.3730 - dense_152_accuracy: 0.2120 - dense_153_accuracy: 0.3630 - dense_154_accuracy: 0.2230 - dense_155_accuracy: 0.2180 - dense_156_accuracy: 0.2230 - dense_157_accuracy: 0.2420 - dense_158_accuracy: 0.2420 - dense_159_accuracy: 0.5140 - dense_160_accuracy: 0.5090\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 15.6934 - dense_151_loss: 1.4201 - dense_152_loss: 1.7504 - dense_153_loss: 1.5065 - dense_154_loss: 1.6614 - dense_155_loss: 1.7196 - dense_156_loss: 1.8925 - dense_157_loss: 1.6556 - dense_158_loss: 1.6623 - dense_159_loss: 1.2018 - dense_160_loss: 1.2231 - dense_151_accuracy: 0.3800 - dense_152_accuracy: 0.2190 - dense_153_accuracy: 0.3640 - dense_154_accuracy: 0.2640 - dense_155_accuracy: 0.2480 - dense_156_accuracy: 0.2290 - dense_157_accuracy: 0.2560 - dense_158_accuracy: 0.2470 - dense_159_accuracy: 0.4970 - dense_160_accuracy: 0.5050\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 15.3005 - dense_151_loss: 1.3738 - dense_152_loss: 1.6891 - dense_153_loss: 1.4758 - dense_154_loss: 1.6436 - dense_155_loss: 1.6703 - dense_156_loss: 1.8529 - dense_157_loss: 1.6152 - dense_158_loss: 1.6236 - dense_159_loss: 1.1740 - dense_160_loss: 1.1822 - dense_151_accuracy: 0.3950 - dense_152_accuracy: 0.2540 - dense_153_accuracy: 0.3680 - dense_154_accuracy: 0.2470 - dense_155_accuracy: 0.2740 - dense_156_accuracy: 0.2170 - dense_157_accuracy: 0.2680 - dense_158_accuracy: 0.2360 - dense_159_accuracy: 0.4920 - dense_160_accuracy: 0.5270\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 15.1306 - dense_151_loss: 1.3781 - dense_152_loss: 1.6776 - dense_153_loss: 1.4665 - dense_154_loss: 1.6214 - dense_155_loss: 1.6476 - dense_156_loss: 1.8204 - dense_157_loss: 1.6005 - dense_158_loss: 1.6087 - dense_159_loss: 1.1439 - dense_160_loss: 1.1659 - dense_151_accuracy: 0.3680 - dense_152_accuracy: 0.2450 - dense_153_accuracy: 0.3570 - dense_154_accuracy: 0.2480 - dense_155_accuracy: 0.2370 - dense_156_accuracy: 0.2360 - dense_157_accuracy: 0.2840 - dense_158_accuracy: 0.2260 - dense_159_accuracy: 0.5090 - dense_160_accuracy: 0.5130\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 15.0007 - dense_151_loss: 1.3671 - dense_152_loss: 1.6617 - dense_153_loss: 1.4531 - dense_154_loss: 1.6095 - dense_155_loss: 1.6389 - dense_156_loss: 1.8018 - dense_157_loss: 1.5857 - dense_158_loss: 1.5977 - dense_159_loss: 1.1381 - dense_160_loss: 1.1471 - dense_151_accuracy: 0.3910 - dense_152_accuracy: 0.2430 - dense_153_accuracy: 0.3630 - dense_154_accuracy: 0.2370 - dense_155_accuracy: 0.2590 - dense_156_accuracy: 0.2430 - dense_157_accuracy: 0.2680 - dense_158_accuracy: 0.2380 - dense_159_accuracy: 0.5130 - dense_160_accuracy: 0.5170\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 14.9652 - dense_151_loss: 1.3689 - dense_152_loss: 1.6645 - dense_153_loss: 1.4503 - dense_154_loss: 1.6040 - dense_155_loss: 1.6281 - dense_156_loss: 1.7980 - dense_157_loss: 1.5838 - dense_158_loss: 1.5962 - dense_159_loss: 1.1331 - dense_160_loss: 1.1384 - dense_151_accuracy: 0.3860 - dense_152_accuracy: 0.2580 - dense_153_accuracy: 0.3670 - dense_154_accuracy: 0.2670 - dense_155_accuracy: 0.2610 - dense_156_accuracy: 0.2390 - dense_157_accuracy: 0.2710 - dense_158_accuracy: 0.2500 - dense_159_accuracy: 0.5050 - dense_160_accuracy: 0.5280\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 15.0201 - dense_151_loss: 1.3704 - dense_152_loss: 1.6678 - dense_153_loss: 1.4546 - dense_154_loss: 1.6080 - dense_155_loss: 1.6454 - dense_156_loss: 1.8075 - dense_157_loss: 1.5874 - dense_158_loss: 1.6024 - dense_159_loss: 1.1361 - dense_160_loss: 1.1405 - dense_151_accuracy: 0.3790 - dense_152_accuracy: 0.2340 - dense_153_accuracy: 0.3570 - dense_154_accuracy: 0.2540 - dense_155_accuracy: 0.2490 - dense_156_accuracy: 0.2380 - dense_157_accuracy: 0.2860 - dense_158_accuracy: 0.2430 - dense_159_accuracy: 0.5160 - dense_160_accuracy: 0.5250\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 14.9162 - dense_151_loss: 1.3657 - dense_152_loss: 1.6509 - dense_153_loss: 1.4500 - dense_154_loss: 1.6035 - dense_155_loss: 1.6291 - dense_156_loss: 1.7931 - dense_157_loss: 1.5763 - dense_158_loss: 1.5901 - dense_159_loss: 1.1291 - dense_160_loss: 1.1285 - dense_151_accuracy: 0.3800 - dense_152_accuracy: 0.2330 - dense_153_accuracy: 0.3500 - dense_154_accuracy: 0.2260 - dense_155_accuracy: 0.2520 - dense_156_accuracy: 0.1930 - dense_157_accuracy: 0.2650 - dense_158_accuracy: 0.2210 - dense_159_accuracy: 0.4970 - dense_160_accuracy: 0.5320\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC00D7B1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC00D7B1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC007F8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC007F8948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 15ms/step - loss: 0.2495\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2472\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2439\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2394\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2326\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2226\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2104\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1917\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1689\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1426\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC007F8318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC007F8318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8/8 [==============================] - 3s 8ms/step - loss: 24.9584 - dense_164_loss: 2.4881 - dense_165_loss: 2.4553 - dense_166_loss: 2.4715 - dense_167_loss: 2.3858 - dense_168_loss: 2.9263 - dense_169_loss: 2.5175 - dense_170_loss: 2.5912 - dense_171_loss: 2.4232 - dense_172_loss: 2.1608 - dense_173_loss: 2.5385 - dense_164_accuracy: 0.1520 - dense_165_accuracy: 0.1280 - dense_166_accuracy: 0.1680 - dense_167_accuracy: 0.2220 - dense_168_accuracy: 0.0880 - dense_169_accuracy: 0.1500 - dense_170_accuracy: 0.0940 - dense_171_accuracy: 0.1160 - dense_172_accuracy: 0.1940 - dense_173_accuracy: 0.1220\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 23.4937 - dense_164_loss: 2.3127 - dense_165_loss: 2.2791 - dense_166_loss: 2.2708 - dense_167_loss: 2.2206 - dense_168_loss: 2.8117 - dense_169_loss: 2.4663 - dense_170_loss: 2.4470 - dense_171_loss: 2.2558 - dense_172_loss: 2.0114 - dense_173_loss: 2.4183 - dense_164_accuracy: 0.1680 - dense_165_accuracy: 0.2160 - dense_166_accuracy: 0.1780 - dense_167_accuracy: 0.2600 - dense_168_accuracy: 0.1120 - dense_169_accuracy: 0.1660 - dense_170_accuracy: 0.1220 - dense_171_accuracy: 0.1380 - dense_172_accuracy: 0.2240 - dense_173_accuracy: 0.1380\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 22.0320 - dense_164_loss: 2.1468 - dense_165_loss: 2.1326 - dense_166_loss: 2.1696 - dense_167_loss: 2.1133 - dense_168_loss: 2.6256 - dense_169_loss: 2.3429 - dense_170_loss: 2.2904 - dense_171_loss: 2.0803 - dense_172_loss: 1.8778 - dense_173_loss: 2.2527 - dense_164_accuracy: 0.2560 - dense_165_accuracy: 0.2220 - dense_166_accuracy: 0.2020 - dense_167_accuracy: 0.2720 - dense_168_accuracy: 0.1280 - dense_169_accuracy: 0.1480 - dense_170_accuracy: 0.1900 - dense_171_accuracy: 0.2220 - dense_172_accuracy: 0.2580 - dense_173_accuracy: 0.2260\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 20.7170 - dense_164_loss: 2.0227 - dense_165_loss: 2.0209 - dense_166_loss: 2.0186 - dense_167_loss: 1.9749 - dense_168_loss: 2.4445 - dense_169_loss: 2.1822 - dense_170_loss: 2.1814 - dense_171_loss: 1.9458 - dense_172_loss: 1.7925 - dense_173_loss: 2.1335 - dense_164_accuracy: 0.2980 - dense_165_accuracy: 0.2140 - dense_166_accuracy: 0.2320 - dense_167_accuracy: 0.3340 - dense_168_accuracy: 0.1580 - dense_169_accuracy: 0.1880 - dense_170_accuracy: 0.2220 - dense_171_accuracy: 0.2640 - dense_172_accuracy: 0.3320 - dense_173_accuracy: 0.2480\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 19.8438 - dense_164_loss: 1.9151 - dense_165_loss: 1.9259 - dense_166_loss: 1.9584 - dense_167_loss: 1.8642 - dense_168_loss: 2.3138 - dense_169_loss: 2.0810 - dense_170_loss: 2.1133 - dense_171_loss: 1.8672 - dense_172_loss: 1.7252 - dense_173_loss: 2.0796 - dense_164_accuracy: 0.3200 - dense_165_accuracy: 0.2880 - dense_166_accuracy: 0.2540 - dense_167_accuracy: 0.3420 - dense_168_accuracy: 0.1900 - dense_169_accuracy: 0.2280 - dense_170_accuracy: 0.2120 - dense_171_accuracy: 0.2960 - dense_172_accuracy: 0.3480 - dense_173_accuracy: 0.2260\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 19.3738 - dense_164_loss: 1.8322 - dense_165_loss: 1.8791 - dense_166_loss: 1.9183 - dense_167_loss: 1.8152 - dense_168_loss: 2.2380 - dense_169_loss: 2.0275 - dense_170_loss: 2.0780 - dense_171_loss: 1.8293 - dense_172_loss: 1.7151 - dense_173_loss: 2.0411 - dense_164_accuracy: 0.3320 - dense_165_accuracy: 0.2940 - dense_166_accuracy: 0.2880 - dense_167_accuracy: 0.3320 - dense_168_accuracy: 0.2380 - dense_169_accuracy: 0.2340 - dense_170_accuracy: 0.2400 - dense_171_accuracy: 0.2960 - dense_172_accuracy: 0.3500 - dense_173_accuracy: 0.2620\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 18.5023 - dense_164_loss: 1.7170 - dense_165_loss: 1.8027 - dense_166_loss: 1.8436 - dense_167_loss: 1.7293 - dense_168_loss: 2.1150 - dense_169_loss: 1.9389 - dense_170_loss: 1.9874 - dense_171_loss: 1.7451 - dense_172_loss: 1.6568 - dense_173_loss: 1.9664 - dense_164_accuracy: 0.3680 - dense_165_accuracy: 0.3420 - dense_166_accuracy: 0.2840 - dense_167_accuracy: 0.3380 - dense_168_accuracy: 0.2360 - dense_169_accuracy: 0.2560 - dense_170_accuracy: 0.2420 - dense_171_accuracy: 0.3300 - dense_172_accuracy: 0.3680 - dense_173_accuracy: 0.2640\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 17.4062 - dense_164_loss: 1.5916 - dense_165_loss: 1.6850 - dense_166_loss: 1.7329 - dense_167_loss: 1.6554 - dense_168_loss: 1.9788 - dense_169_loss: 1.8137 - dense_170_loss: 1.8697 - dense_171_loss: 1.6705 - dense_172_loss: 1.5815 - dense_173_loss: 1.8270 - dense_164_accuracy: 0.3660 - dense_165_accuracy: 0.3460 - dense_166_accuracy: 0.2800 - dense_167_accuracy: 0.3260 - dense_168_accuracy: 0.2680 - dense_169_accuracy: 0.2900 - dense_170_accuracy: 0.2800 - dense_171_accuracy: 0.3300 - dense_172_accuracy: 0.3680 - dense_173_accuracy: 0.2940\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 16.6981 - dense_164_loss: 1.5254 - dense_165_loss: 1.5846 - dense_166_loss: 1.6947 - dense_167_loss: 1.5976 - dense_168_loss: 1.9038 - dense_169_loss: 1.7531 - dense_170_loss: 1.7852 - dense_171_loss: 1.6213 - dense_172_loss: 1.5088 - dense_173_loss: 1.7236 - dense_164_accuracy: 0.3760 - dense_165_accuracy: 0.3660 - dense_166_accuracy: 0.3040 - dense_167_accuracy: 0.2920 - dense_168_accuracy: 0.2780 - dense_169_accuracy: 0.2720 - dense_170_accuracy: 0.2660 - dense_171_accuracy: 0.3260 - dense_172_accuracy: 0.3820 - dense_173_accuracy: 0.3140\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 16.3617 - dense_164_loss: 1.5033 - dense_165_loss: 1.5396 - dense_166_loss: 1.6629 - dense_167_loss: 1.5504 - dense_168_loss: 1.8621 - dense_169_loss: 1.7280 - dense_170_loss: 1.7276 - dense_171_loss: 1.6052 - dense_172_loss: 1.4937 - dense_173_loss: 1.6889 - dense_164_accuracy: 0.3700 - dense_165_accuracy: 0.3720 - dense_166_accuracy: 0.3280 - dense_167_accuracy: 0.3560 - dense_168_accuracy: 0.2640 - dense_169_accuracy: 0.2800 - dense_170_accuracy: 0.2900 - dense_171_accuracy: 0.3380 - dense_172_accuracy: 0.3860 - dense_173_accuracy: 0.3080\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC733E58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC733E58B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC733C3D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC733C3D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8/8 [==============================] - 3s 8ms/step - loss: 24.7528 - dense_174_loss: 2.4852 - dense_175_loss: 2.3747 - dense_176_loss: 2.4578 - dense_177_loss: 2.3343 - dense_178_loss: 2.9535 - dense_179_loss: 2.5545 - dense_180_loss: 2.5669 - dense_181_loss: 2.3557 - dense_182_loss: 2.1776 - dense_183_loss: 2.4927 - dense_174_accuracy: 0.1360 - dense_175_accuracy: 0.1760 - dense_176_accuracy: 0.1260 - dense_177_accuracy: 0.2240 - dense_178_accuracy: 0.0700 - dense_179_accuracy: 0.1040 - dense_180_accuracy: 0.1260 - dense_181_accuracy: 0.1360 - dense_182_accuracy: 0.1640 - dense_183_accuracy: 0.1560\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 22.7308 - dense_174_loss: 2.2220 - dense_175_loss: 2.1725 - dense_176_loss: 2.2332 - dense_177_loss: 2.1100 - dense_178_loss: 2.7177 - dense_179_loss: 2.4327 - dense_180_loss: 2.3520 - dense_181_loss: 2.1606 - dense_182_loss: 1.9672 - dense_183_loss: 2.3629 - dense_174_accuracy: 0.2320 - dense_175_accuracy: 0.2160 - dense_176_accuracy: 0.2240 - dense_177_accuracy: 0.2740 - dense_178_accuracy: 0.1240 - dense_179_accuracy: 0.1420 - dense_180_accuracy: 0.1540 - dense_181_accuracy: 0.1780 - dense_182_accuracy: 0.2360 - dense_183_accuracy: 0.1720\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 20.8690 - dense_174_loss: 1.9951 - dense_175_loss: 2.0063 - dense_176_loss: 2.0545 - dense_177_loss: 1.9297 - dense_178_loss: 2.4976 - dense_179_loss: 2.2421 - dense_180_loss: 2.2236 - dense_181_loss: 1.9312 - dense_182_loss: 1.8421 - dense_183_loss: 2.1470 - dense_174_accuracy: 0.3100 - dense_175_accuracy: 0.2440 - dense_176_accuracy: 0.2660 - dense_177_accuracy: 0.3260 - dense_178_accuracy: 0.1720 - dense_179_accuracy: 0.1500 - dense_180_accuracy: 0.2020 - dense_181_accuracy: 0.2920 - dense_182_accuracy: 0.3080 - dense_183_accuracy: 0.2480\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 19.3427 - dense_174_loss: 1.7962 - dense_175_loss: 1.8261 - dense_176_loss: 1.9405 - dense_177_loss: 1.8270 - dense_178_loss: 2.2756 - dense_179_loss: 2.0539 - dense_180_loss: 2.0333 - dense_181_loss: 1.8463 - dense_182_loss: 1.7373 - dense_183_loss: 2.0065 - dense_174_accuracy: 0.3140 - dense_175_accuracy: 0.3240 - dense_176_accuracy: 0.2860 - dense_177_accuracy: 0.3180 - dense_178_accuracy: 0.2260 - dense_179_accuracy: 0.2420 - dense_180_accuracy: 0.2380 - dense_181_accuracy: 0.2880 - dense_182_accuracy: 0.3200 - dense_183_accuracy: 0.3140\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 17.9390 - dense_174_loss: 1.6678 - dense_175_loss: 1.6796 - dense_176_loss: 1.8031 - dense_177_loss: 1.7152 - dense_178_loss: 2.1049 - dense_179_loss: 1.8782 - dense_180_loss: 1.9174 - dense_181_loss: 1.6958 - dense_182_loss: 1.6276 - dense_183_loss: 1.8492 - dense_174_accuracy: 0.3500 - dense_175_accuracy: 0.3540 - dense_176_accuracy: 0.2960 - dense_177_accuracy: 0.3420 - dense_178_accuracy: 0.2420 - dense_179_accuracy: 0.2540 - dense_180_accuracy: 0.2400 - dense_181_accuracy: 0.3260 - dense_182_accuracy: 0.3640 - dense_183_accuracy: 0.3040\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 17.0615 - dense_174_loss: 1.5501 - dense_175_loss: 1.6064 - dense_176_loss: 1.7267 - dense_177_loss: 1.6346 - dense_178_loss: 1.9753 - dense_179_loss: 1.7946 - dense_180_loss: 1.8285 - dense_181_loss: 1.6378 - dense_182_loss: 1.5479 - dense_183_loss: 1.7596 - dense_174_accuracy: 0.3800 - dense_175_accuracy: 0.3680 - dense_176_accuracy: 0.3180 - dense_177_accuracy: 0.3380 - dense_178_accuracy: 0.2460 - dense_179_accuracy: 0.2420 - dense_180_accuracy: 0.2520 - dense_181_accuracy: 0.3220 - dense_182_accuracy: 0.3860 - dense_183_accuracy: 0.3000\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 16.7387 - dense_174_loss: 1.5523 - dense_175_loss: 1.5755 - dense_176_loss: 1.7018 - dense_177_loss: 1.6067 - dense_178_loss: 1.9285 - dense_179_loss: 1.7569 - dense_180_loss: 1.7667 - dense_181_loss: 1.6083 - dense_182_loss: 1.5168 - dense_183_loss: 1.7252 - dense_174_accuracy: 0.3660 - dense_175_accuracy: 0.3460 - dense_176_accuracy: 0.2920 - dense_177_accuracy: 0.3060 - dense_178_accuracy: 0.2460 - dense_179_accuracy: 0.2900 - dense_180_accuracy: 0.2500 - dense_181_accuracy: 0.3020 - dense_182_accuracy: 0.3680 - dense_183_accuracy: 0.3120\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 16.6569 - dense_174_loss: 1.5176 - dense_175_loss: 1.5567 - dense_176_loss: 1.6820 - dense_177_loss: 1.6082 - dense_178_loss: 1.9060 - dense_179_loss: 1.7705 - dense_180_loss: 1.7763 - dense_181_loss: 1.6185 - dense_182_loss: 1.5197 - dense_183_loss: 1.7014 - dense_174_accuracy: 0.3880 - dense_175_accuracy: 0.3560 - dense_176_accuracy: 0.3200 - dense_177_accuracy: 0.2980 - dense_178_accuracy: 0.2660 - dense_179_accuracy: 0.2720 - dense_180_accuracy: 0.2700 - dense_181_accuracy: 0.3040 - dense_182_accuracy: 0.3640 - dense_183_accuracy: 0.3160\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 16.4045 - dense_174_loss: 1.4985 - dense_175_loss: 1.5354 - dense_176_loss: 1.6654 - dense_177_loss: 1.5828 - dense_178_loss: 1.8763 - dense_179_loss: 1.7411 - dense_180_loss: 1.7386 - dense_181_loss: 1.5850 - dense_182_loss: 1.4986 - dense_183_loss: 1.6828 - dense_174_accuracy: 0.3680 - dense_175_accuracy: 0.3440 - dense_176_accuracy: 0.3180 - dense_177_accuracy: 0.3200 - dense_178_accuracy: 0.2600 - dense_179_accuracy: 0.2800 - dense_180_accuracy: 0.2720 - dense_181_accuracy: 0.3440 - dense_182_accuracy: 0.3760 - dense_183_accuracy: 0.3080\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 16.2516 - dense_174_loss: 1.4902 - dense_175_loss: 1.5203 - dense_176_loss: 1.6734 - dense_177_loss: 1.5574 - dense_178_loss: 1.8397 - dense_179_loss: 1.7243 - dense_180_loss: 1.7109 - dense_181_loss: 1.5989 - dense_182_loss: 1.4677 - dense_183_loss: 1.6686 - dense_174_accuracy: 0.3640 - dense_175_accuracy: 0.3540 - dense_176_accuracy: 0.3000 - dense_177_accuracy: 0.3500 - dense_178_accuracy: 0.2560 - dense_179_accuracy: 0.2760 - dense_180_accuracy: 0.2840 - dense_181_accuracy: 0.3380 - dense_182_accuracy: 0.3780 - dense_183_accuracy: 0.3260\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC00F5DE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC00F5DE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72DBD828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72DBD828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 19ms/step - loss: 0.2050\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.0242\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.0057\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.0050\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.0048\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.0047\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.0047\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.0047\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.0046\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.0046\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00E68828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00E68828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 4s 9ms/step - loss: 17.3645 - dense_187_loss: 1.5530 - dense_188_loss: 1.7139 - dense_189_loss: 1.8395 - dense_190_loss: 1.6947 - dense_191_loss: 1.9531 - dense_192_loss: 1.7726 - dense_193_loss: 1.6994 - dense_194_loss: 1.7253 - dense_195_loss: 1.6108 - dense_196_loss: 1.8022 - dense_187_accuracy: 0.4016 - dense_188_accuracy: 0.2712 - dense_189_accuracy: 0.2248 - dense_190_accuracy: 0.2850 - dense_191_accuracy: 0.2034 - dense_192_accuracy: 0.2602 - dense_193_accuracy: 0.3240 - dense_194_accuracy: 0.2632 - dense_195_accuracy: 0.3562 - dense_196_accuracy: 0.2254\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 15.4057 - dense_187_loss: 1.3197 - dense_188_loss: 1.5523 - dense_189_loss: 1.6472 - dense_190_loss: 1.5197 - dense_191_loss: 1.7138 - dense_192_loss: 1.5508 - dense_193_loss: 1.4940 - dense_194_loss: 1.5604 - dense_195_loss: 1.4189 - dense_196_loss: 1.6288 - dense_187_accuracy: 0.4710 - dense_188_accuracy: 0.2990 - dense_189_accuracy: 0.2322 - dense_190_accuracy: 0.2882 - dense_191_accuracy: 0.2234 - dense_192_accuracy: 0.2816 - dense_193_accuracy: 0.3486 - dense_194_accuracy: 0.2840 - dense_195_accuracy: 0.3990 - dense_196_accuracy: 0.2310\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 15.3466 - dense_187_loss: 1.3132 - dense_188_loss: 1.5479 - dense_189_loss: 1.6406 - dense_190_loss: 1.5121 - dense_191_loss: 1.7099 - dense_192_loss: 1.5438 - dense_193_loss: 1.4870 - dense_194_loss: 1.5539 - dense_195_loss: 1.4118 - dense_196_loss: 1.6263 - dense_187_accuracy: 0.4722 - dense_188_accuracy: 0.2950 - dense_189_accuracy: 0.2364 - dense_190_accuracy: 0.3036 - dense_191_accuracy: 0.2330 - dense_192_accuracy: 0.2980 - dense_193_accuracy: 0.3554 - dense_194_accuracy: 0.2998 - dense_195_accuracy: 0.3976 - dense_196_accuracy: 0.2312\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 15.2969 - dense_187_loss: 1.3104 - dense_188_loss: 1.5380 - dense_189_loss: 1.6337 - dense_190_loss: 1.5119 - dense_191_loss: 1.7026 - dense_192_loss: 1.5399 - dense_193_loss: 1.4815 - dense_194_loss: 1.5530 - dense_195_loss: 1.4093 - dense_196_loss: 1.6166 - dense_187_accuracy: 0.4714 - dense_188_accuracy: 0.3010 - dense_189_accuracy: 0.2376 - dense_190_accuracy: 0.3056 - dense_191_accuracy: 0.2480 - dense_192_accuracy: 0.2972 - dense_193_accuracy: 0.3566 - dense_194_accuracy: 0.3018 - dense_195_accuracy: 0.3972 - dense_196_accuracy: 0.2600\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 15.2848 - dense_187_loss: 1.3102 - dense_188_loss: 1.5400 - dense_189_loss: 1.6322 - dense_190_loss: 1.5086 - dense_191_loss: 1.7022 - dense_192_loss: 1.5404 - dense_193_loss: 1.4783 - dense_194_loss: 1.5464 - dense_195_loss: 1.4090 - dense_196_loss: 1.6173 - dense_187_accuracy: 0.4740 - dense_188_accuracy: 0.3014 - dense_189_accuracy: 0.2416 - dense_190_accuracy: 0.2980 - dense_191_accuracy: 0.2350 - dense_192_accuracy: 0.2950 - dense_193_accuracy: 0.3560 - dense_194_accuracy: 0.2998 - dense_195_accuracy: 0.4028 - dense_196_accuracy: 0.2460\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 15.2668 - dense_187_loss: 1.3092 - dense_188_loss: 1.5364 - dense_189_loss: 1.6331 - dense_190_loss: 1.5116 - dense_191_loss: 1.7021 - dense_192_loss: 1.5374 - dense_193_loss: 1.4775 - dense_194_loss: 1.5439 - dense_195_loss: 1.4031 - dense_196_loss: 1.6124 - dense_187_accuracy: 0.4754 - dense_188_accuracy: 0.3132 - dense_189_accuracy: 0.2354 - dense_190_accuracy: 0.3084 - dense_191_accuracy: 0.2408 - dense_192_accuracy: 0.3076 - dense_193_accuracy: 0.3604 - dense_194_accuracy: 0.3072 - dense_195_accuracy: 0.4008 - dense_196_accuracy: 0.2474\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 15.1970 - dense_187_loss: 1.3048 - dense_188_loss: 1.5339 - dense_189_loss: 1.6246 - dense_190_loss: 1.4959 - dense_191_loss: 1.6925 - dense_192_loss: 1.5325 - dense_193_loss: 1.4720 - dense_194_loss: 1.5413 - dense_195_loss: 1.3947 - dense_196_loss: 1.6047 - dense_187_accuracy: 0.4746 - dense_188_accuracy: 0.3174 - dense_189_accuracy: 0.2530 - dense_190_accuracy: 0.3178 - dense_191_accuracy: 0.2486 - dense_192_accuracy: 0.3062 - dense_193_accuracy: 0.3534 - dense_194_accuracy: 0.3094 - dense_195_accuracy: 0.4090 - dense_196_accuracy: 0.2604\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 15.1859 - dense_187_loss: 1.3050 - dense_188_loss: 1.5362 - dense_189_loss: 1.6212 - dense_190_loss: 1.4962 - dense_191_loss: 1.6910 - dense_192_loss: 1.5278 - dense_193_loss: 1.4689 - dense_194_loss: 1.5397 - dense_195_loss: 1.3925 - dense_196_loss: 1.6076 - dense_187_accuracy: 0.4780 - dense_188_accuracy: 0.3160 - dense_189_accuracy: 0.2590 - dense_190_accuracy: 0.3128 - dense_191_accuracy: 0.2610 - dense_192_accuracy: 0.3038 - dense_193_accuracy: 0.3706 - dense_194_accuracy: 0.3182 - dense_195_accuracy: 0.4088 - dense_196_accuracy: 0.2580\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 15.0730 - dense_187_loss: 1.2939 - dense_188_loss: 1.5225 - dense_189_loss: 1.6093 - dense_190_loss: 1.4817 - dense_191_loss: 1.6757 - dense_192_loss: 1.5182 - dense_193_loss: 1.4612 - dense_194_loss: 1.5292 - dense_195_loss: 1.3871 - dense_196_loss: 1.5944 - dense_187_accuracy: 0.4816 - dense_188_accuracy: 0.3190 - dense_189_accuracy: 0.2670 - dense_190_accuracy: 0.3238 - dense_191_accuracy: 0.2598 - dense_192_accuracy: 0.3220 - dense_193_accuracy: 0.3662 - dense_194_accuracy: 0.3238 - dense_195_accuracy: 0.4078 - dense_196_accuracy: 0.2664\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 14.9910 - dense_187_loss: 1.2823 - dense_188_loss: 1.5103 - dense_189_loss: 1.6045 - dense_190_loss: 1.4777 - dense_191_loss: 1.6680 - dense_192_loss: 1.5102 - dense_193_loss: 1.4542 - dense_194_loss: 1.5193 - dense_195_loss: 1.3768 - dense_196_loss: 1.5876 - dense_187_accuracy: 0.4850 - dense_188_accuracy: 0.3336 - dense_189_accuracy: 0.2698 - dense_190_accuracy: 0.3284 - dense_191_accuracy: 0.2738 - dense_192_accuracy: 0.3140 - dense_193_accuracy: 0.3826 - dense_194_accuracy: 0.3272 - dense_195_accuracy: 0.4228 - dense_196_accuracy: 0.2822\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC76624828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC76624828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00DE4D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00DE4D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 5s 8ms/step - loss: 17.2981 - dense_197_loss: 1.5241 - dense_198_loss: 1.7321 - dense_199_loss: 1.8308 - dense_200_loss: 1.6921 - dense_201_loss: 1.9568 - dense_202_loss: 1.7388 - dense_203_loss: 1.6997 - dense_204_loss: 1.7196 - dense_205_loss: 1.5981 - dense_206_loss: 1.8059 - dense_197_accuracy: 0.4152 - dense_198_accuracy: 0.2744 - dense_199_accuracy: 0.2154 - dense_200_accuracy: 0.2758 - dense_201_accuracy: 0.2144 - dense_202_accuracy: 0.2516 - dense_203_accuracy: 0.3188 - dense_204_accuracy: 0.2632 - dense_205_accuracy: 0.3560 - dense_206_accuracy: 0.2186\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 15.3517 - dense_197_loss: 1.3165 - dense_198_loss: 1.5469 - dense_199_loss: 1.6410 - dense_200_loss: 1.5157 - dense_201_loss: 1.7075 - dense_202_loss: 1.5479 - dense_203_loss: 1.4875 - dense_204_loss: 1.5543 - dense_205_loss: 1.4099 - dense_206_loss: 1.6244 - dense_197_accuracy: 0.4676 - dense_198_accuracy: 0.2966 - dense_199_accuracy: 0.2376 - dense_200_accuracy: 0.2970 - dense_201_accuracy: 0.2348 - dense_202_accuracy: 0.2952 - dense_203_accuracy: 0.3612 - dense_204_accuracy: 0.2886 - dense_205_accuracy: 0.4006 - dense_206_accuracy: 0.2424\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 15.3100 - dense_197_loss: 1.3121 - dense_198_loss: 1.5441 - dense_199_loss: 1.6353 - dense_200_loss: 1.5087 - dense_201_loss: 1.7062 - dense_202_loss: 1.5399 - dense_203_loss: 1.4829 - dense_204_loss: 1.5490 - dense_205_loss: 1.4109 - dense_206_loss: 1.6210 - dense_197_accuracy: 0.4760 - dense_198_accuracy: 0.2932 - dense_199_accuracy: 0.2388 - dense_200_accuracy: 0.3044 - dense_201_accuracy: 0.2312 - dense_202_accuracy: 0.3026 - dense_203_accuracy: 0.3518 - dense_204_accuracy: 0.2962 - dense_205_accuracy: 0.3982 - dense_206_accuracy: 0.2360\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 15.3041 - dense_197_loss: 1.3110 - dense_198_loss: 1.5440 - dense_199_loss: 1.6345 - dense_200_loss: 1.5091 - dense_201_loss: 1.7024 - dense_202_loss: 1.5403 - dense_203_loss: 1.4825 - dense_204_loss: 1.5526 - dense_205_loss: 1.4083 - dense_206_loss: 1.6194 - dense_197_accuracy: 0.4696 - dense_198_accuracy: 0.2964 - dense_199_accuracy: 0.2328 - dense_200_accuracy: 0.3046 - dense_201_accuracy: 0.2396 - dense_202_accuracy: 0.3012 - dense_203_accuracy: 0.3574 - dense_204_accuracy: 0.3044 - dense_205_accuracy: 0.3994 - dense_206_accuracy: 0.2398\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 15.2862 - dense_197_loss: 1.3115 - dense_198_loss: 1.5436 - dense_199_loss: 1.6306 - dense_200_loss: 1.5090 - dense_201_loss: 1.7019 - dense_202_loss: 1.5392 - dense_203_loss: 1.4789 - dense_204_loss: 1.5479 - dense_205_loss: 1.4071 - dense_206_loss: 1.6164 - dense_197_accuracy: 0.4704 - dense_198_accuracy: 0.2968 - dense_199_accuracy: 0.2230 - dense_200_accuracy: 0.2996 - dense_201_accuracy: 0.2306 - dense_202_accuracy: 0.2956 - dense_203_accuracy: 0.3494 - dense_204_accuracy: 0.2950 - dense_205_accuracy: 0.4002 - dense_206_accuracy: 0.2382\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 15.2690 - dense_197_loss: 1.3095 - dense_198_loss: 1.5405 - dense_199_loss: 1.6309 - dense_200_loss: 1.5045 - dense_201_loss: 1.6987 - dense_202_loss: 1.5374 - dense_203_loss: 1.4808 - dense_204_loss: 1.5460 - dense_205_loss: 1.4054 - dense_206_loss: 1.6153 - dense_197_accuracy: 0.4762 - dense_198_accuracy: 0.3014 - dense_199_accuracy: 0.2262 - dense_200_accuracy: 0.3014 - dense_201_accuracy: 0.2308 - dense_202_accuracy: 0.2990 - dense_203_accuracy: 0.3576 - dense_204_accuracy: 0.2910 - dense_205_accuracy: 0.3996 - dense_206_accuracy: 0.2412\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 15.2460 - dense_197_loss: 1.3044 - dense_198_loss: 1.5374 - dense_199_loss: 1.6267 - dense_200_loss: 1.5040 - dense_201_loss: 1.6980 - dense_202_loss: 1.5349 - dense_203_loss: 1.4764 - dense_204_loss: 1.5450 - dense_205_loss: 1.4029 - dense_206_loss: 1.6163 - dense_197_accuracy: 0.4762 - dense_198_accuracy: 0.2920 - dense_199_accuracy: 0.2396 - dense_200_accuracy: 0.3000 - dense_201_accuracy: 0.2266 - dense_202_accuracy: 0.2930 - dense_203_accuracy: 0.3598 - dense_204_accuracy: 0.2960 - dense_205_accuracy: 0.4052 - dense_206_accuracy: 0.2446\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 15.2405 - dense_197_loss: 1.3055 - dense_198_loss: 1.5373 - dense_199_loss: 1.6277 - dense_200_loss: 1.5041 - dense_201_loss: 1.6971 - dense_202_loss: 1.5327 - dense_203_loss: 1.4757 - dense_204_loss: 1.5445 - dense_205_loss: 1.4038 - dense_206_loss: 1.6122 - dense_197_accuracy: 0.4754 - dense_198_accuracy: 0.3062 - dense_199_accuracy: 0.2332 - dense_200_accuracy: 0.3054 - dense_201_accuracy: 0.2240 - dense_202_accuracy: 0.2962 - dense_203_accuracy: 0.3558 - dense_204_accuracy: 0.2970 - dense_205_accuracy: 0.4006 - dense_206_accuracy: 0.2364\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 15.2408 - dense_197_loss: 1.3067 - dense_198_loss: 1.5374 - dense_199_loss: 1.6266 - dense_200_loss: 1.5036 - dense_201_loss: 1.6954 - dense_202_loss: 1.5324 - dense_203_loss: 1.4758 - dense_204_loss: 1.5444 - dense_205_loss: 1.4046 - dense_206_loss: 1.6140 - dense_197_accuracy: 0.4756 - dense_198_accuracy: 0.3004 - dense_199_accuracy: 0.2322 - dense_200_accuracy: 0.2996 - dense_201_accuracy: 0.2456 - dense_202_accuracy: 0.3000 - dense_203_accuracy: 0.3594 - dense_204_accuracy: 0.2978 - dense_205_accuracy: 0.3986 - dense_206_accuracy: 0.2386\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 15.2341 - dense_197_loss: 1.3053 - dense_198_loss: 1.5377 - dense_199_loss: 1.6266 - dense_200_loss: 1.5006 - dense_201_loss: 1.6960 - dense_202_loss: 1.5336 - dense_203_loss: 1.4735 - dense_204_loss: 1.5451 - dense_205_loss: 1.4027 - dense_206_loss: 1.6129 - dense_197_accuracy: 0.4750 - dense_198_accuracy: 0.3054 - dense_199_accuracy: 0.2452 - dense_200_accuracy: 0.3110 - dense_201_accuracy: 0.2382 - dense_202_accuracy: 0.3062 - dense_203_accuracy: 0.3562 - dense_204_accuracy: 0.3046 - dense_205_accuracy: 0.4034 - dense_206_accuracy: 0.2324\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC071709D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC071709D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC009AC678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC009AC678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 14ms/step - loss: 0.1336\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0062\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0051\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.0049\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0048\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0044\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0039\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0035\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0034\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 2s 15ms/step - loss: 0.0033\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC71AA7AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC71AA7AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "157/157 [==============================] - 5s 7ms/step - loss: 16.8847 - dense_210_loss: 1.5128 - dense_211_loss: 1.9497 - dense_212_loss: 1.5478 - dense_213_loss: 1.6761 - dense_214_loss: 1.9403 - dense_215_loss: 1.9489 - dense_216_loss: 1.7139 - dense_217_loss: 1.5934 - dense_218_loss: 1.5225 - dense_219_loss: 1.4793 - dense_210_accuracy: 0.3560 - dense_211_accuracy: 0.2010 - dense_212_accuracy: 0.3857 - dense_213_accuracy: 0.2817 - dense_214_accuracy: 0.2093 - dense_215_accuracy: 0.2079 - dense_216_accuracy: 0.2698 - dense_217_accuracy: 0.2960 - dense_218_accuracy: 0.3660 - dense_219_accuracy: 0.3612\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.3047 - dense_210_loss: 1.3082 - dense_211_loss: 1.6201 - dense_212_loss: 1.3045 - dense_213_loss: 1.4512 - dense_214_loss: 1.6244 - dense_215_loss: 1.6318 - dense_216_loss: 1.4650 - dense_217_loss: 1.3634 - dense_218_loss: 1.2526 - dense_219_loss: 1.2834 - dense_210_accuracy: 0.4247 - dense_211_accuracy: 0.2361 - dense_212_accuracy: 0.4301 - dense_213_accuracy: 0.3198 - dense_214_accuracy: 0.2421 - dense_215_accuracy: 0.2426 - dense_216_accuracy: 0.3125 - dense_217_accuracy: 0.3130 - dense_218_accuracy: 0.4077 - dense_219_accuracy: 0.3964\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.2483 - dense_210_loss: 1.3035 - dense_211_loss: 1.6119 - dense_212_loss: 1.2994 - dense_213_loss: 1.4487 - dense_214_loss: 1.6167 - dense_215_loss: 1.6267 - dense_216_loss: 1.4591 - dense_217_loss: 1.3602 - dense_218_loss: 1.2465 - dense_219_loss: 1.2756 - dense_210_accuracy: 0.4204 - dense_211_accuracy: 0.2456 - dense_212_accuracy: 0.4311 - dense_213_accuracy: 0.3184 - dense_214_accuracy: 0.2395 - dense_215_accuracy: 0.2388 - dense_216_accuracy: 0.3055 - dense_217_accuracy: 0.3186 - dense_218_accuracy: 0.4146 - dense_219_accuracy: 0.4029\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.2168 - dense_210_loss: 1.3012 - dense_211_loss: 1.6105 - dense_212_loss: 1.2961 - dense_213_loss: 1.4454 - dense_214_loss: 1.6143 - dense_215_loss: 1.6213 - dense_216_loss: 1.4551 - dense_217_loss: 1.3562 - dense_218_loss: 1.2414 - dense_219_loss: 1.2752 - dense_210_accuracy: 0.4206 - dense_211_accuracy: 0.2451 - dense_212_accuracy: 0.4305 - dense_213_accuracy: 0.3212 - dense_214_accuracy: 0.2428 - dense_215_accuracy: 0.2485 - dense_216_accuracy: 0.3131 - dense_217_accuracy: 0.3245 - dense_218_accuracy: 0.4112 - dense_219_accuracy: 0.4045\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.1921 - dense_210_loss: 1.2993 - dense_211_loss: 1.6073 - dense_212_loss: 1.2922 - dense_213_loss: 1.4410 - dense_214_loss: 1.6125 - dense_215_loss: 1.6201 - dense_216_loss: 1.4531 - dense_217_loss: 1.3536 - dense_218_loss: 1.2415 - dense_219_loss: 1.2716 - dense_210_accuracy: 0.4277 - dense_211_accuracy: 0.2495 - dense_212_accuracy: 0.4356 - dense_213_accuracy: 0.3220 - dense_214_accuracy: 0.2425 - dense_215_accuracy: 0.2430 - dense_216_accuracy: 0.3088 - dense_217_accuracy: 0.3281 - dense_218_accuracy: 0.4126 - dense_219_accuracy: 0.4101\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.2136 - dense_210_loss: 1.3015 - dense_211_loss: 1.6101 - dense_212_loss: 1.2940 - dense_213_loss: 1.4436 - dense_214_loss: 1.6123 - dense_215_loss: 1.6219 - dense_216_loss: 1.4526 - dense_217_loss: 1.3555 - dense_218_loss: 1.2471 - dense_219_loss: 1.2750 - dense_210_accuracy: 0.4235 - dense_211_accuracy: 0.2466 - dense_212_accuracy: 0.4356 - dense_213_accuracy: 0.3279 - dense_214_accuracy: 0.2465 - dense_215_accuracy: 0.2448 - dense_216_accuracy: 0.3112 - dense_217_accuracy: 0.3266 - dense_218_accuracy: 0.4136 - dense_219_accuracy: 0.4069\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.1344 - dense_210_loss: 1.2955 - dense_211_loss: 1.6003 - dense_212_loss: 1.2843 - dense_213_loss: 1.4359 - dense_214_loss: 1.6043 - dense_215_loss: 1.6131 - dense_216_loss: 1.4487 - dense_217_loss: 1.3487 - dense_218_loss: 1.2363 - dense_219_loss: 1.2672 - dense_210_accuracy: 0.4261 - dense_211_accuracy: 0.2547 - dense_212_accuracy: 0.4418 - dense_213_accuracy: 0.3323 - dense_214_accuracy: 0.2519 - dense_215_accuracy: 0.2536 - dense_216_accuracy: 0.3168 - dense_217_accuracy: 0.3257 - dense_218_accuracy: 0.4178 - dense_219_accuracy: 0.4108\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.0768 - dense_210_loss: 1.2905 - dense_211_loss: 1.5921 - dense_212_loss: 1.2856 - dense_213_loss: 1.4284 - dense_214_loss: 1.5981 - dense_215_loss: 1.6049 - dense_216_loss: 1.4397 - dense_217_loss: 1.3418 - dense_218_loss: 1.2321 - dense_219_loss: 1.2636 - dense_210_accuracy: 0.4281 - dense_211_accuracy: 0.2687 - dense_212_accuracy: 0.4394 - dense_213_accuracy: 0.3411 - dense_214_accuracy: 0.2606 - dense_215_accuracy: 0.2664 - dense_216_accuracy: 0.3229 - dense_217_accuracy: 0.3356 - dense_218_accuracy: 0.4211 - dense_219_accuracy: 0.4185\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.0810 - dense_210_loss: 1.2885 - dense_211_loss: 1.5935 - dense_212_loss: 1.2861 - dense_213_loss: 1.4289 - dense_214_loss: 1.5979 - dense_215_loss: 1.6037 - dense_216_loss: 1.4420 - dense_217_loss: 1.3439 - dense_218_loss: 1.2332 - dense_219_loss: 1.2633 - dense_210_accuracy: 0.4303 - dense_211_accuracy: 0.2703 - dense_212_accuracy: 0.4377 - dense_213_accuracy: 0.3385 - dense_214_accuracy: 0.2658 - dense_215_accuracy: 0.2669 - dense_216_accuracy: 0.3223 - dense_217_accuracy: 0.3385 - dense_218_accuracy: 0.4309 - dense_219_accuracy: 0.4146\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 13.9946 - dense_210_loss: 1.2875 - dense_211_loss: 1.5815 - dense_212_loss: 1.2773 - dense_213_loss: 1.4180 - dense_214_loss: 1.5859 - dense_215_loss: 1.5944 - dense_216_loss: 1.4332 - dense_217_loss: 1.3352 - dense_218_loss: 1.2247 - dense_219_loss: 1.2568 - dense_210_accuracy: 0.4291 - dense_211_accuracy: 0.2763 - dense_212_accuracy: 0.4436 - dense_213_accuracy: 0.3474 - dense_214_accuracy: 0.2734 - dense_215_accuracy: 0.2725 - dense_216_accuracy: 0.3348 - dense_217_accuracy: 0.3474 - dense_218_accuracy: 0.4295 - dense_219_accuracy: 0.4193\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC0087BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC0087BB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00F1A1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC00F1A1F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "157/157 [==============================] - 4s 9ms/step - loss: 16.1915 - dense_220_loss: 1.4689 - dense_221_loss: 1.8641 - dense_222_loss: 1.4873 - dense_223_loss: 1.6082 - dense_224_loss: 1.8550 - dense_225_loss: 1.8605 - dense_226_loss: 1.6422 - dense_227_loss: 1.5367 - dense_228_loss: 1.4421 - dense_229_loss: 1.4265 - dense_220_accuracy: 0.3761 - dense_221_accuracy: 0.2162 - dense_222_accuracy: 0.3925 - dense_223_accuracy: 0.3002 - dense_224_accuracy: 0.2171 - dense_225_accuracy: 0.2145 - dense_226_accuracy: 0.2839 - dense_227_accuracy: 0.3015 - dense_228_accuracy: 0.3754 - dense_229_accuracy: 0.3719\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.3831 - dense_220_loss: 1.3146 - dense_221_loss: 1.6301 - dense_222_loss: 1.3124 - dense_223_loss: 1.4596 - dense_224_loss: 1.6345 - dense_225_loss: 1.6410 - dense_226_loss: 1.4714 - dense_227_loss: 1.3706 - dense_228_loss: 1.2605 - dense_229_loss: 1.2884 - dense_220_accuracy: 0.4209 - dense_221_accuracy: 0.2425 - dense_222_accuracy: 0.4284 - dense_223_accuracy: 0.3173 - dense_224_accuracy: 0.2429 - dense_225_accuracy: 0.2391 - dense_226_accuracy: 0.3046 - dense_227_accuracy: 0.3140 - dense_228_accuracy: 0.4047 - dense_229_accuracy: 0.3997\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.2895 - dense_220_loss: 1.3064 - dense_221_loss: 1.6184 - dense_222_loss: 1.3012 - dense_223_loss: 1.4532 - dense_224_loss: 1.6225 - dense_225_loss: 1.6296 - dense_226_loss: 1.4628 - dense_227_loss: 1.3646 - dense_228_loss: 1.2493 - dense_229_loss: 1.2815 - dense_220_accuracy: 0.4202 - dense_221_accuracy: 0.2393 - dense_222_accuracy: 0.4339 - dense_223_accuracy: 0.3172 - dense_224_accuracy: 0.2346 - dense_225_accuracy: 0.2350 - dense_226_accuracy: 0.3053 - dense_227_accuracy: 0.3121 - dense_228_accuracy: 0.4093 - dense_229_accuracy: 0.3972\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.1939 - dense_220_loss: 1.2967 - dense_221_loss: 1.6080 - dense_222_loss: 1.2940 - dense_223_loss: 1.4430 - dense_224_loss: 1.6098 - dense_225_loss: 1.6184 - dense_226_loss: 1.4533 - dense_227_loss: 1.3534 - dense_228_loss: 1.2424 - dense_229_loss: 1.2748 - dense_220_accuracy: 0.4234 - dense_221_accuracy: 0.2399 - dense_222_accuracy: 0.4330 - dense_223_accuracy: 0.3142 - dense_224_accuracy: 0.2419 - dense_225_accuracy: 0.2400 - dense_226_accuracy: 0.3120 - dense_227_accuracy: 0.3196 - dense_228_accuracy: 0.4100 - dense_229_accuracy: 0.3982\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.1912 - dense_220_loss: 1.2996 - dense_221_loss: 1.6069 - dense_222_loss: 1.2928 - dense_223_loss: 1.4424 - dense_224_loss: 1.6100 - dense_225_loss: 1.6191 - dense_226_loss: 1.4509 - dense_227_loss: 1.3540 - dense_228_loss: 1.2413 - dense_229_loss: 1.2742 - dense_220_accuracy: 0.4246 - dense_221_accuracy: 0.2445 - dense_222_accuracy: 0.4355 - dense_223_accuracy: 0.3215 - dense_224_accuracy: 0.2424 - dense_225_accuracy: 0.2384 - dense_226_accuracy: 0.3176 - dense_227_accuracy: 0.3142 - dense_228_accuracy: 0.4099 - dense_229_accuracy: 0.4016\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 14.1578 - dense_220_loss: 1.2959 - dense_221_loss: 1.6045 - dense_222_loss: 1.2881 - dense_223_loss: 1.4389 - dense_224_loss: 1.6066 - dense_225_loss: 1.6144 - dense_226_loss: 1.4509 - dense_227_loss: 1.3494 - dense_228_loss: 1.2382 - dense_229_loss: 1.2710 - dense_220_accuracy: 0.4239 - dense_221_accuracy: 0.2355 - dense_222_accuracy: 0.4374 - dense_223_accuracy: 0.3151 - dense_224_accuracy: 0.2378 - dense_225_accuracy: 0.2400 - dense_226_accuracy: 0.3076 - dense_227_accuracy: 0.3178 - dense_228_accuracy: 0.4088 - dense_229_accuracy: 0.3947\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 14.1924 - dense_220_loss: 1.2963 - dense_221_loss: 1.6086 - dense_222_loss: 1.2917 - dense_223_loss: 1.4414 - dense_224_loss: 1.6115 - dense_225_loss: 1.6201 - dense_226_loss: 1.4536 - dense_227_loss: 1.3511 - dense_228_loss: 1.2443 - dense_229_loss: 1.2739 - dense_220_accuracy: 0.4256 - dense_221_accuracy: 0.2390 - dense_222_accuracy: 0.4337 - dense_223_accuracy: 0.3202 - dense_224_accuracy: 0.2384 - dense_225_accuracy: 0.2385 - dense_226_accuracy: 0.3102 - dense_227_accuracy: 0.3178 - dense_228_accuracy: 0.4058 - dense_229_accuracy: 0.3946\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 14.1422 - dense_220_loss: 1.2927 - dense_221_loss: 1.6028 - dense_222_loss: 1.2868 - dense_223_loss: 1.4370 - dense_224_loss: 1.6049 - dense_225_loss: 1.6143 - dense_226_loss: 1.4485 - dense_227_loss: 1.3472 - dense_228_loss: 1.2374 - dense_229_loss: 1.2707 - dense_220_accuracy: 0.4228 - dense_221_accuracy: 0.2461 - dense_222_accuracy: 0.4351 - dense_223_accuracy: 0.3194 - dense_224_accuracy: 0.2421 - dense_225_accuracy: 0.2437 - dense_226_accuracy: 0.3128 - dense_227_accuracy: 0.3214 - dense_228_accuracy: 0.4123 - dense_229_accuracy: 0.4026\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.1486 - dense_220_loss: 1.2933 - dense_221_loss: 1.6043 - dense_222_loss: 1.2879 - dense_223_loss: 1.4374 - dense_224_loss: 1.6054 - dense_225_loss: 1.6143 - dense_226_loss: 1.4474 - dense_227_loss: 1.3489 - dense_228_loss: 1.2388 - dense_229_loss: 1.2710 - dense_220_accuracy: 0.4221 - dense_221_accuracy: 0.2417 - dense_222_accuracy: 0.4339 - dense_223_accuracy: 0.3168 - dense_224_accuracy: 0.2424 - dense_225_accuracy: 0.2422 - dense_226_accuracy: 0.3079 - dense_227_accuracy: 0.3174 - dense_228_accuracy: 0.4089 - dense_229_accuracy: 0.4023\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 14.1328 - dense_220_loss: 1.2944 - dense_221_loss: 1.6017 - dense_222_loss: 1.2855 - dense_223_loss: 1.4363 - dense_224_loss: 1.6045 - dense_225_loss: 1.6122 - dense_226_loss: 1.4458 - dense_227_loss: 1.3480 - dense_228_loss: 1.2363 - dense_229_loss: 1.2682 - dense_220_accuracy: 0.4240 - dense_221_accuracy: 0.2405 - dense_222_accuracy: 0.4359 - dense_223_accuracy: 0.3220 - dense_224_accuracy: 0.2391 - dense_225_accuracy: 0.2412 - dense_226_accuracy: 0.3096 - dense_227_accuracy: 0.3198 - dense_228_accuracy: 0.4074 - dense_229_accuracy: 0.4042\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC068B6D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC068B6D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72D950D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC72D950D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jostm7\\Anaconda3\\envs\\replearn\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 15ms/step - loss: 0.2207\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.0478\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.0078\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.0063\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.0060\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.0059\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.0058\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.0058\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.0058\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.0057\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC6831FC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC6831FC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 5s 8ms/step - loss: 20.5407 - dense_233_loss: 1.9483 - dense_234_loss: 1.9932 - dense_235_loss: 2.1136 - dense_236_loss: 1.9612 - dense_237_loss: 2.3750 - dense_238_loss: 2.0943 - dense_239_loss: 2.1103 - dense_240_loss: 1.9749 - dense_241_loss: 1.8522 - dense_242_loss: 2.1177 - dense_233_accuracy: 0.3148 - dense_234_accuracy: 0.2530 - dense_235_accuracy: 0.2204 - dense_236_accuracy: 0.2580 - dense_237_accuracy: 0.1564 - dense_238_accuracy: 0.2192 - dense_239_accuracy: 0.2152 - dense_240_accuracy: 0.2438 - dense_241_accuracy: 0.2988 - dense_242_accuracy: 0.1940\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.3718 - dense_233_loss: 1.5992 - dense_234_loss: 1.6975 - dense_235_loss: 1.8018 - dense_236_loss: 1.6849 - dense_237_loss: 1.9706 - dense_238_loss: 1.7639 - dense_239_loss: 1.7729 - dense_240_loss: 1.7075 - dense_241_loss: 1.5535 - dense_242_loss: 1.8200 - dense_233_accuracy: 0.4044 - dense_234_accuracy: 0.3182 - dense_235_accuracy: 0.2618 - dense_236_accuracy: 0.2778 - dense_237_accuracy: 0.2198 - dense_238_accuracy: 0.2668 - dense_239_accuracy: 0.2666 - dense_240_accuracy: 0.2902 - dense_241_accuracy: 0.3914 - dense_242_accuracy: 0.2374\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.1634 - dense_233_loss: 1.5805 - dense_234_loss: 1.6809 - dense_235_loss: 1.7816 - dense_236_loss: 1.6546 - dense_237_loss: 1.9513 - dense_238_loss: 1.7430 - dense_239_loss: 1.7564 - dense_240_loss: 1.6883 - dense_241_loss: 1.5310 - dense_242_loss: 1.7957 - dense_233_accuracy: 0.4104 - dense_234_accuracy: 0.3134 - dense_235_accuracy: 0.2598 - dense_236_accuracy: 0.2894 - dense_237_accuracy: 0.2148 - dense_238_accuracy: 0.2698 - dense_239_accuracy: 0.2826 - dense_240_accuracy: 0.2910 - dense_241_accuracy: 0.3920 - dense_242_accuracy: 0.2324\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.1139 - dense_233_loss: 1.5732 - dense_234_loss: 1.6731 - dense_235_loss: 1.7733 - dense_236_loss: 1.6542 - dense_237_loss: 1.9391 - dense_238_loss: 1.7325 - dense_239_loss: 1.7539 - dense_240_loss: 1.6915 - dense_241_loss: 1.5305 - dense_242_loss: 1.7924 - dense_233_accuracy: 0.4148 - dense_234_accuracy: 0.3344 - dense_235_accuracy: 0.2722 - dense_236_accuracy: 0.2950 - dense_237_accuracy: 0.2294 - dense_238_accuracy: 0.2834 - dense_239_accuracy: 0.2916 - dense_240_accuracy: 0.2960 - dense_241_accuracy: 0.4010 - dense_242_accuracy: 0.2426\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.0899 - dense_233_loss: 1.5749 - dense_234_loss: 1.6677 - dense_235_loss: 1.7694 - dense_236_loss: 1.6487 - dense_237_loss: 1.9415 - dense_238_loss: 1.7293 - dense_239_loss: 1.7528 - dense_240_loss: 1.6846 - dense_241_loss: 1.5324 - dense_242_loss: 1.7886 - dense_233_accuracy: 0.4112 - dense_234_accuracy: 0.3320 - dense_235_accuracy: 0.2818 - dense_236_accuracy: 0.2970 - dense_237_accuracy: 0.2324 - dense_238_accuracy: 0.2800 - dense_239_accuracy: 0.2894 - dense_240_accuracy: 0.3004 - dense_241_accuracy: 0.3920 - dense_242_accuracy: 0.2400\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 17.0553 - dense_233_loss: 1.5740 - dense_234_loss: 1.6633 - dense_235_loss: 1.7653 - dense_236_loss: 1.6454 - dense_237_loss: 1.9350 - dense_238_loss: 1.7281 - dense_239_loss: 1.7489 - dense_240_loss: 1.6802 - dense_241_loss: 1.5312 - dense_242_loss: 1.7840 - dense_233_accuracy: 0.4026 - dense_234_accuracy: 0.3366 - dense_235_accuracy: 0.2806 - dense_236_accuracy: 0.3016 - dense_237_accuracy: 0.2358 - dense_238_accuracy: 0.2764 - dense_239_accuracy: 0.2876 - dense_240_accuracy: 0.2978 - dense_241_accuracy: 0.3952 - dense_242_accuracy: 0.2440\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 16.9157 - dense_233_loss: 1.5527 - dense_234_loss: 1.6501 - dense_235_loss: 1.7515 - dense_236_loss: 1.6377 - dense_237_loss: 1.9212 - dense_238_loss: 1.7145 - dense_239_loss: 1.7381 - dense_240_loss: 1.6634 - dense_241_loss: 1.5173 - dense_242_loss: 1.7691 - dense_233_accuracy: 0.4128 - dense_234_accuracy: 0.3414 - dense_235_accuracy: 0.2926 - dense_236_accuracy: 0.3058 - dense_237_accuracy: 0.2584 - dense_238_accuracy: 0.2968 - dense_239_accuracy: 0.2996 - dense_240_accuracy: 0.3214 - dense_241_accuracy: 0.3944 - dense_242_accuracy: 0.2662\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 16.8292 - dense_233_loss: 1.5500 - dense_234_loss: 1.6409 - dense_235_loss: 1.7423 - dense_236_loss: 1.6287 - dense_237_loss: 1.9070 - dense_238_loss: 1.7082 - dense_239_loss: 1.7296 - dense_240_loss: 1.6562 - dense_241_loss: 1.5057 - dense_242_loss: 1.7607 - dense_233_accuracy: 0.4180 - dense_234_accuracy: 0.3534 - dense_235_accuracy: 0.3056 - dense_236_accuracy: 0.3160 - dense_237_accuracy: 0.2776 - dense_238_accuracy: 0.3004 - dense_239_accuracy: 0.3120 - dense_240_accuracy: 0.3296 - dense_241_accuracy: 0.4098 - dense_242_accuracy: 0.2926\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 16.6724 - dense_233_loss: 1.5349 - dense_234_loss: 1.6336 - dense_235_loss: 1.7237 - dense_236_loss: 1.6098 - dense_237_loss: 1.8911 - dense_238_loss: 1.6874 - dense_239_loss: 1.7093 - dense_240_loss: 1.6425 - dense_241_loss: 1.4949 - dense_242_loss: 1.7452 - dense_233_accuracy: 0.4264 - dense_234_accuracy: 0.3596 - dense_235_accuracy: 0.3210 - dense_236_accuracy: 0.3366 - dense_237_accuracy: 0.2822 - dense_238_accuracy: 0.3198 - dense_239_accuracy: 0.3286 - dense_240_accuracy: 0.3368 - dense_241_accuracy: 0.4104 - dense_242_accuracy: 0.2982\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 16.4995 - dense_233_loss: 1.5184 - dense_234_loss: 1.6168 - dense_235_loss: 1.7013 - dense_236_loss: 1.5982 - dense_237_loss: 1.8682 - dense_238_loss: 1.6722 - dense_239_loss: 1.6941 - dense_240_loss: 1.6251 - dense_241_loss: 1.4786 - dense_242_loss: 1.7267 - dense_233_accuracy: 0.4364 - dense_234_accuracy: 0.3664 - dense_235_accuracy: 0.3430 - dense_236_accuracy: 0.3502 - dense_237_accuracy: 0.3086 - dense_238_accuracy: 0.3332 - dense_239_accuracy: 0.3444 - dense_240_accuracy: 0.3454 - dense_241_accuracy: 0.4228 - dense_242_accuracy: 0.3162\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC017615E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC017615E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC01A0CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001AC01A0CD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 4s 8ms/step - loss: 19.9754 - dense_243_loss: 1.8923 - dense_244_loss: 1.9414 - dense_245_loss: 2.0397 - dense_246_loss: 1.9147 - dense_247_loss: 2.3132 - dense_248_loss: 2.0260 - dense_249_loss: 2.0590 - dense_250_loss: 1.9246 - dense_251_loss: 1.7866 - dense_252_loss: 2.0779 - dense_243_accuracy: 0.3300 - dense_244_accuracy: 0.2730 - dense_245_accuracy: 0.2378 - dense_246_accuracy: 0.2552 - dense_247_accuracy: 0.1720 - dense_248_accuracy: 0.2226 - dense_249_accuracy: 0.2342 - dense_250_accuracy: 0.2500 - dense_251_accuracy: 0.3226 - dense_252_accuracy: 0.2010\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.3258 - dense_243_loss: 1.6005 - dense_244_loss: 1.6912 - dense_245_loss: 1.7955 - dense_246_loss: 1.6688 - dense_247_loss: 1.9746 - dense_248_loss: 1.7595 - dense_249_loss: 1.7714 - dense_250_loss: 1.7046 - dense_251_loss: 1.5482 - dense_252_loss: 1.8116 - dense_243_accuracy: 0.4118 - dense_244_accuracy: 0.3186 - dense_245_accuracy: 0.2594 - dense_246_accuracy: 0.2916 - dense_247_accuracy: 0.2186 - dense_248_accuracy: 0.2692 - dense_249_accuracy: 0.2906 - dense_250_accuracy: 0.2818 - dense_251_accuracy: 0.3892 - dense_252_accuracy: 0.2276\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.1593 - dense_243_loss: 1.5755 - dense_244_loss: 1.6801 - dense_245_loss: 1.7779 - dense_246_loss: 1.6583 - dense_247_loss: 1.9517 - dense_248_loss: 1.7439 - dense_249_loss: 1.7583 - dense_250_loss: 1.6838 - dense_251_loss: 1.5365 - dense_252_loss: 1.7933 - dense_243_accuracy: 0.4120 - dense_244_accuracy: 0.3194 - dense_245_accuracy: 0.2594 - dense_246_accuracy: 0.2918 - dense_247_accuracy: 0.2084 - dense_248_accuracy: 0.2690 - dense_249_accuracy: 0.2754 - dense_250_accuracy: 0.2882 - dense_251_accuracy: 0.3924 - dense_252_accuracy: 0.2320\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.1129 - dense_243_loss: 1.5700 - dense_244_loss: 1.6762 - dense_245_loss: 1.7753 - dense_246_loss: 1.6526 - dense_247_loss: 1.9471 - dense_248_loss: 1.7371 - dense_249_loss: 1.7516 - dense_250_loss: 1.6847 - dense_251_loss: 1.5296 - dense_252_loss: 1.7888 - dense_243_accuracy: 0.4166 - dense_244_accuracy: 0.3264 - dense_245_accuracy: 0.2636 - dense_246_accuracy: 0.2888 - dense_247_accuracy: 0.2168 - dense_248_accuracy: 0.2754 - dense_249_accuracy: 0.2848 - dense_250_accuracy: 0.2860 - dense_251_accuracy: 0.3888 - dense_252_accuracy: 0.2332\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 17.0803 - dense_243_loss: 1.5697 - dense_244_loss: 1.6702 - dense_245_loss: 1.7698 - dense_246_loss: 1.6467 - dense_247_loss: 1.9433 - dense_248_loss: 1.7310 - dense_249_loss: 1.7516 - dense_250_loss: 1.6808 - dense_251_loss: 1.5305 - dense_252_loss: 1.7867 - dense_243_accuracy: 0.4102 - dense_244_accuracy: 0.3286 - dense_245_accuracy: 0.2622 - dense_246_accuracy: 0.2994 - dense_247_accuracy: 0.2200 - dense_248_accuracy: 0.2684 - dense_249_accuracy: 0.2844 - dense_250_accuracy: 0.2936 - dense_251_accuracy: 0.3920 - dense_252_accuracy: 0.2330\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 17.0802 - dense_243_loss: 1.5687 - dense_244_loss: 1.6704 - dense_245_loss: 1.7718 - dense_246_loss: 1.6486 - dense_247_loss: 1.9412 - dense_248_loss: 1.7319 - dense_249_loss: 1.7481 - dense_250_loss: 1.6850 - dense_251_loss: 1.5290 - dense_252_loss: 1.7855 - dense_243_accuracy: 0.4126 - dense_244_accuracy: 0.3188 - dense_245_accuracy: 0.2648 - dense_246_accuracy: 0.2880 - dense_247_accuracy: 0.2210 - dense_248_accuracy: 0.2728 - dense_249_accuracy: 0.2772 - dense_250_accuracy: 0.2908 - dense_251_accuracy: 0.3876 - dense_252_accuracy: 0.2390\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.0453 - dense_243_loss: 1.5660 - dense_244_loss: 1.6693 - dense_245_loss: 1.7697 - dense_246_loss: 1.6462 - dense_247_loss: 1.9354 - dense_248_loss: 1.7265 - dense_249_loss: 1.7479 - dense_250_loss: 1.6771 - dense_251_loss: 1.5248 - dense_252_loss: 1.7824 - dense_243_accuracy: 0.4170 - dense_244_accuracy: 0.3306 - dense_245_accuracy: 0.2680 - dense_246_accuracy: 0.2974 - dense_247_accuracy: 0.2262 - dense_248_accuracy: 0.2744 - dense_249_accuracy: 0.2772 - dense_250_accuracy: 0.2948 - dense_251_accuracy: 0.3952 - dense_252_accuracy: 0.2420\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 17.0327 - dense_243_loss: 1.5623 - dense_244_loss: 1.6650 - dense_245_loss: 1.7670 - dense_246_loss: 1.6437 - dense_247_loss: 1.9337 - dense_248_loss: 1.7301 - dense_249_loss: 1.7453 - dense_250_loss: 1.6782 - dense_251_loss: 1.5276 - dense_252_loss: 1.7799 - dense_243_accuracy: 0.4144 - dense_244_accuracy: 0.3326 - dense_245_accuracy: 0.2622 - dense_246_accuracy: 0.2984 - dense_247_accuracy: 0.2254 - dense_248_accuracy: 0.2806 - dense_249_accuracy: 0.2836 - dense_250_accuracy: 0.3064 - dense_251_accuracy: 0.3962 - dense_252_accuracy: 0.2552\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 17.0051 - dense_243_loss: 1.5589 - dense_244_loss: 1.6654 - dense_245_loss: 1.7651 - dense_246_loss: 1.6438 - dense_247_loss: 1.9325 - dense_248_loss: 1.7251 - dense_249_loss: 1.7400 - dense_250_loss: 1.6746 - dense_251_loss: 1.5230 - dense_252_loss: 1.7766 - dense_243_accuracy: 0.4170 - dense_244_accuracy: 0.3278 - dense_245_accuracy: 0.2690 - dense_246_accuracy: 0.2974 - dense_247_accuracy: 0.2302 - dense_248_accuracy: 0.2738 - dense_249_accuracy: 0.2786 - dense_250_accuracy: 0.2986 - dense_251_accuracy: 0.3970 - dense_252_accuracy: 0.2386\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 16.9733 - dense_243_loss: 1.5583 - dense_244_loss: 1.6607 - dense_245_loss: 1.7586 - dense_246_loss: 1.6380 - dense_247_loss: 1.9285 - dense_248_loss: 1.7240 - dense_249_loss: 1.7393 - dense_250_loss: 1.6727 - dense_251_loss: 1.5192 - dense_252_loss: 1.7740 - dense_243_accuracy: 0.4164 - dense_244_accuracy: 0.3306 - dense_245_accuracy: 0.2776 - dense_246_accuracy: 0.3044 - dense_247_accuracy: 0.2334 - dense_248_accuracy: 0.2742 - dense_249_accuracy: 0.2858 - dense_250_accuracy: 0.3098 - dense_251_accuracy: 0.3998 - dense_252_accuracy: 0.2466\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC01B9F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001AC01B9F708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get all files from Folder Iteration ending with 'json.gz'\n",
    "event_log_path = '/logs/iteration/*.json.gz'\n",
    "git_path = git.Repo(\".\", search_parent_directories=True).git.rev_parse(\"--show-toplevel\")\n",
    "final_path = git_path+event_log_path\n",
    "files = glob.glob(final_path)\n",
    "\n",
    "column_names = {    0:'Method', \n",
    "                    1:'b1',\n",
    "                    2:'b2',\n",
    "                    3:'b3',\n",
    "                    4:'b4',\n",
    "                    5:'b5',\n",
    "                    6:'b6',\n",
    "                    7:'b7'}\n",
    "combined_results = pd.DataFrame()\n",
    "combined_results.rename(columns = column_names, inplace = True) \n",
    "\n",
    "for filepath in files:\n",
    "\n",
    "    # load eventlog \n",
    "    # event log configuration\n",
    "    event_log_path = filepath\n",
    "    file_name = os.path.basename(filepath)\n",
    "\n",
    "    case_attributes = None # auto-detect attributes\n",
    "    event_attributes = ['concept:name', 'user'] # use activity name and user\n",
    "    true_cluster_label = 'cluster'\n",
    "\n",
    "    # load file\n",
    "    event_log = EventLog(file_name, case_attributes=case_attributes, event_attributes=event_attributes, true_cluster_label=true_cluster_label)\n",
    "    event_log.load(event_log_path, False)\n",
    "    event_log.preprocess()\n",
    "\n",
    "\n",
    "    # hyperparameters\n",
    "    n_epochs = 10\n",
    "    n_batch_size = 64\n",
    "    n_clusters = 5\n",
    "    vector_size = 32\n",
    "    hyperparameters = [n_epochs,n_batch_size,n_clusters,vector_size]\n",
    "    \n",
    "    # get combined results for current file and add filename as first column \n",
    "    current_combined_results = Iteration.get_combined_results(event_log,hyperparameters,column_names)\n",
    "    current_combined_results.insert(loc=0, column='Filename', value=file_name)\n",
    "\n",
    "    # add current_combined_results to overall combined results df \n",
    "    combined_results = combined_results.append(current_combined_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Method</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_5000_10_20_5_3_1-0.1-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.419124</td>\n",
       "      <td>0.44466</td>\n",
       "      <td>0.555623</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38651</td>\n",
       "      <td>0.523406</td>\n",
       "      <td>{1: 2444, 2: 88, 3: 878, 4: 1542, 5: 48}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_5000_10_20_5_3_1-0.1-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.429597</td>\n",
       "      <td>0.558633</td>\n",
       "      <td>0.584802</td>\n",
       "      <td>0</td>\n",
       "      <td>0.561198</td>\n",
       "      <td>0.556091</td>\n",
       "      <td>{1: 1108, 2: 544, 3: 853, 4: 895, 5: 1600}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_5000_10_20_5_3_1-0.1-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.391404</td>\n",
       "      <td>0.499087</td>\n",
       "      <td>0.54788</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496726</td>\n",
       "      <td>0.501469</td>\n",
       "      <td>{1: 1108, 2: 354, 3: 1043, 4: 1600, 5: 895}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_5000_10_20_5_3_1-0.1-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>0.044706</td>\n",
       "      <td>0.329714</td>\n",
       "      <td>0.523436</td>\n",
       "      <td>0</td>\n",
       "      <td>0.282293</td>\n",
       "      <td>0.396285</td>\n",
       "      <td>{1: 392, 2: 372, 3: 526, 4: 374, 5: 3336}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_5000_10_20_5_3_1-0.1-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.99696</td>\n",
       "      <td>0.992792</td>\n",
       "      <td>0.996812</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992822</td>\n",
       "      <td>0.992762</td>\n",
       "      <td>{1: 1788, 2: 612, 3: 807, 4: 901, 5: 892}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_5000_10_20_5_3_1-0.1-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.992239</td>\n",
       "      <td>0.984646</td>\n",
       "      <td>0.99101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985109</td>\n",
       "      <td>0.984184</td>\n",
       "      <td>{1: 1788, 2: 897, 3: 880, 4: 803, 5: 632}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.4-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.333902</td>\n",
       "      <td>0.367311</td>\n",
       "      <td>0.532924</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416458</td>\n",
       "      <td>0.328539</td>\n",
       "      <td>{1: 240, 2: 295, 3: 129, 4: 63, 5: 273}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.4-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.37523</td>\n",
       "      <td>0.544526</td>\n",
       "      <td>0.59945</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626474</td>\n",
       "      <td>0.481537</td>\n",
       "      <td>{1: 309, 2: 220, 3: 222, 4: 83, 5: 166}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.4-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.574644</td>\n",
       "      <td>0.646719</td>\n",
       "      <td>0.70831</td>\n",
       "      <td>0</td>\n",
       "      <td>0.726941</td>\n",
       "      <td>0.582444</td>\n",
       "      <td>{1: 158, 2: 78, 3: 146, 4: 222, 5: 396}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.4-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>0.196298</td>\n",
       "      <td>0.550673</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15645</td>\n",
       "      <td>0.263381</td>\n",
       "      <td>{1: 24, 2: 50, 3: 60, 4: 42, 5: 824}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.4-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.989589</td>\n",
       "      <td>0.982805</td>\n",
       "      <td>0.992232</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992645</td>\n",
       "      <td>0.973158</td>\n",
       "      <td>{1: 427, 2: 6, 3: 366, 4: 56, 5: 145}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.4-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.99599</td>\n",
       "      <td>0.986669</td>\n",
       "      <td>0.994112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991421</td>\n",
       "      <td>0.981963</td>\n",
       "      <td>{1: 432, 2: 365, 3: 59, 4: 141, 5: 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.5-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.263954</td>\n",
       "      <td>0.346892</td>\n",
       "      <td>0.519418</td>\n",
       "      <td>0</td>\n",
       "      <td>0.382945</td>\n",
       "      <td>0.317043</td>\n",
       "      <td>{1: 239, 2: 335, 3: 78, 4: 54, 5: 294}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.5-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.346919</td>\n",
       "      <td>0.492818</td>\n",
       "      <td>0.575945</td>\n",
       "      <td>0</td>\n",
       "      <td>0.56918</td>\n",
       "      <td>0.434522</td>\n",
       "      <td>{1: 305, 2: 206, 3: 244, 4: 133, 5: 112}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.5-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.474781</td>\n",
       "      <td>0.562492</td>\n",
       "      <td>0.646334</td>\n",
       "      <td>0</td>\n",
       "      <td>0.637994</td>\n",
       "      <td>0.502969</td>\n",
       "      <td>{1: 165, 2: 80, 3: 140, 4: 340, 5: 275}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.5-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>-0.03014</td>\n",
       "      <td>0.141137</td>\n",
       "      <td>0.51745</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116432</td>\n",
       "      <td>0.179148</td>\n",
       "      <td>{1: 75, 2: 37, 3: 801, 4: 43, 5: 44}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.5-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.846497</td>\n",
       "      <td>0.90816</td>\n",
       "      <td>0.916555</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995718</td>\n",
       "      <td>0.834756</td>\n",
       "      <td>{1: 333, 2: 99, 3: 366, 4: 144, 5: 58}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium_1000_10_20_5_3_1-0.5-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>0.990268</td>\n",
       "      <td>0.996038</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991043</td>\n",
       "      <td>0.989494</td>\n",
       "      <td>{1: 432, 2: 364, 3: 146, 4: 57, 5: 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p2p_500_10_20_5_1_1-0.8-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.258863</td>\n",
       "      <td>0.366568</td>\n",
       "      <td>0.500469</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387241</td>\n",
       "      <td>0.34799</td>\n",
       "      <td>{1: 77, 2: 112, 3: 153, 4: 127, 5: 31}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p2p_500_10_20_5_1_1-0.8-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.244757</td>\n",
       "      <td>0.347538</td>\n",
       "      <td>0.490639</td>\n",
       "      <td>0</td>\n",
       "      <td>0.362317</td>\n",
       "      <td>0.333917</td>\n",
       "      <td>{1: 57, 2: 188, 3: 82, 4: 41, 5: 132}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p2p_500_10_20_5_1_1-0.8-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.04789</td>\n",
       "      <td>0.109349</td>\n",
       "      <td>0.308063</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118354</td>\n",
       "      <td>0.101618</td>\n",
       "      <td>{1: 153, 2: 82, 3: 83, 4: 85, 5: 97}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p2p_500_10_20_5_1_1-0.8-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>-0.004501</td>\n",
       "      <td>0.116917</td>\n",
       "      <td>0.475764</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078429</td>\n",
       "      <td>0.229584</td>\n",
       "      <td>{1: 14, 2: 11, 3: 450, 4: 10, 5: 15}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p2p_500_10_20_5_1_1-0.8-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.982913</td>\n",
       "      <td>0.97299</td>\n",
       "      <td>0.984832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96508</td>\n",
       "      <td>0.98103</td>\n",
       "      <td>{1: 100, 2: 72, 3: 196, 4: 131, 5: 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p2p_500_10_20_5_1_1-0.8-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.967043</td>\n",
       "      <td>0.95589</td>\n",
       "      <td>0.974275</td>\n",
       "      <td>0</td>\n",
       "      <td>0.965305</td>\n",
       "      <td>0.946658</td>\n",
       "      <td>{1: 71, 2: 101, 3: 133, 4: 188, 5: 7}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_5000_10_20_5_1_1-0.5-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.597131</td>\n",
       "      <td>0.594571</td>\n",
       "      <td>0.71133</td>\n",
       "      <td>0</td>\n",
       "      <td>0.713602</td>\n",
       "      <td>0.509573</td>\n",
       "      <td>{1: 955, 2: 239, 3: 798, 4: 1737, 5: 1271}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_5000_10_20_5_1_1-0.5-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.896071</td>\n",
       "      <td>0.877863</td>\n",
       "      <td>0.905032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996913</td>\n",
       "      <td>0.784213</td>\n",
       "      <td>{1: 1230, 2: 2423, 3: 606, 4: 294, 5: 447}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_5000_10_20_5_1_1-0.5-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.645112</td>\n",
       "      <td>0.799717</td>\n",
       "      <td>0.772311</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995913</td>\n",
       "      <td>0.6681</td>\n",
       "      <td>{1: 1230, 2: 616, 3: 730, 4: 1407, 5: 1017}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_5000_10_20_5_1_1-0.5-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.201651</td>\n",
       "      <td>0.443248</td>\n",
       "      <td>0</td>\n",
       "      <td>0.226905</td>\n",
       "      <td>0.181455</td>\n",
       "      <td>{1: 2685, 2: 548, 3: 790, 4: 336, 5: 641}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_5000_10_20_5_1_1-0.5-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.893706</td>\n",
       "      <td>0.895669</td>\n",
       "      <td>0.918404</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.811051</td>\n",
       "      <td>{1: 89, 2: 2332, 3: 1230, 4: 426, 5: 923}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small_5000_10_20_5_1_1-0.5-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.996764</td>\n",
       "      <td>0.991488</td>\n",
       "      <td>0.997013</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983119</td>\n",
       "      <td>{1: 2421, 2: 4, 3: 1345, 4: 11, 5: 1219}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_10000_10_20_5_3_1-0.3-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.178657</td>\n",
       "      <td>0.244695</td>\n",
       "      <td>0.375316</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257948</td>\n",
       "      <td>0.232738</td>\n",
       "      <td>{1: 1137, 2: 2054, 3: 2390, 4: 3114, 5: 1305}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_10000_10_20_5_3_1-0.3-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.260238</td>\n",
       "      <td>0.391607</td>\n",
       "      <td>0.478975</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402698</td>\n",
       "      <td>0.38111</td>\n",
       "      <td>{1: 2806, 2: 3726, 3: 1173, 4: 1030, 5: 1265}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_10000_10_20_5_3_1-0.3-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.227857</td>\n",
       "      <td>0.391211</td>\n",
       "      <td>0.474234</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397109</td>\n",
       "      <td>0.385484</td>\n",
       "      <td>{1: 1354, 2: 1050, 3: 1066, 4: 2102, 5: 4428}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_10000_10_20_5_3_1-0.3-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>0.068301</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.299916</td>\n",
       "      <td>0</td>\n",
       "      <td>0.106968</td>\n",
       "      <td>0.098207</td>\n",
       "      <td>{1: 2180, 2: 3583, 3: 1289, 4: 1065, 5: 1883}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_10000_10_20_5_3_1-0.3-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.995614</td>\n",
       "      <td>0.990799</td>\n",
       "      <td>0.99641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.990856</td>\n",
       "      <td>{1: 3050, 2: 2659, 3: 2252, 4: 36, 5: 2003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_10000_10_20_5_3_1-0.3-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>0.990728</td>\n",
       "      <td>0.996411</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990646</td>\n",
       "      <td>0.99081</td>\n",
       "      <td>{1: 2004, 2: 36, 3: 2247, 4: 2662, 5: 3051}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_5000_10_20_5_1_1-1.0-1.json.gz</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.080231</td>\n",
       "      <td>0.122571</td>\n",
       "      <td>0.325481</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118206</td>\n",
       "      <td>0.127271</td>\n",
       "      <td>{1: 629, 2: 1963, 3: 627, 4: 267, 5: 1514}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_5000_10_20_5_1_1-1.0-1.json.gz</td>\n",
       "      <td>TRACE2VEC</td>\n",
       "      <td>0.289056</td>\n",
       "      <td>0.441972</td>\n",
       "      <td>0.504116</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435377</td>\n",
       "      <td>0.44877</td>\n",
       "      <td>{1: 567, 2: 554, 3: 604, 4: 1382, 5: 1893}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_5000_10_20_5_1_1-1.0-1.json.gz</td>\n",
       "      <td>CASE2VEC E</td>\n",
       "      <td>0.28533</td>\n",
       "      <td>0.435407</td>\n",
       "      <td>0.501381</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428274</td>\n",
       "      <td>0.442781</td>\n",
       "      <td>{1: 724, 2: 532, 3: 472, 4: 1390, 5: 1882}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_5000_10_20_5_1_1-1.0-1.json.gz</td>\n",
       "      <td>CASE2VEC E+C</td>\n",
       "      <td>-0.00071</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>0.377707</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.043861</td>\n",
       "      <td>{1: 30, 2: 4938, 3: 10, 4: 5, 5: 17}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_5000_10_20_5_1_1-1.0-1.json.gz</td>\n",
       "      <td>GRU</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.978016</td>\n",
       "      <td>0.989293</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97803</td>\n",
       "      <td>0.978002</td>\n",
       "      <td>{1: 1403, 2: 866, 3: 989, 4: 320, 5: 1422}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wide_5000_10_20_5_1_1-1.0-1.json.gz</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.98595</td>\n",
       "      <td>0.978546</td>\n",
       "      <td>0.989297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978413</td>\n",
       "      <td>0.978678</td>\n",
       "      <td>{1: 1403, 2: 1422, 3: 866, 4: 991, 5: 318}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Filename        Method        b1        b2  \\\n",
       "0   large_5000_10_20_5_3_1-0.1-1.json.gz   Autoencoder  0.419124   0.44466   \n",
       "0   large_5000_10_20_5_3_1-0.1-1.json.gz     TRACE2VEC  0.429597  0.558633   \n",
       "0   large_5000_10_20_5_3_1-0.1-1.json.gz    CASE2VEC E  0.391404  0.499087   \n",
       "0   large_5000_10_20_5_3_1-0.1-1.json.gz  CASE2VEC E+C  0.044706  0.329714   \n",
       "0   large_5000_10_20_5_3_1-0.1-1.json.gz           GRU   0.99696  0.992792   \n",
       "0   large_5000_10_20_5_3_1-0.1-1.json.gz          LSTM  0.992239  0.984646   \n",
       "0  medium_1000_10_20_5_3_1-0.4-1.json.gz   Autoencoder  0.333902  0.367311   \n",
       "0  medium_1000_10_20_5_3_1-0.4-1.json.gz     TRACE2VEC   0.37523  0.544526   \n",
       "0  medium_1000_10_20_5_3_1-0.4-1.json.gz    CASE2VEC E  0.574644  0.646719   \n",
       "0  medium_1000_10_20_5_3_1-0.4-1.json.gz  CASE2VEC E+C -0.008301  0.196298   \n",
       "0  medium_1000_10_20_5_3_1-0.4-1.json.gz           GRU  0.989589  0.982805   \n",
       "0  medium_1000_10_20_5_3_1-0.4-1.json.gz          LSTM   0.99599  0.986669   \n",
       "0  medium_1000_10_20_5_3_1-0.5-1.json.gz   Autoencoder  0.263954  0.346892   \n",
       "0  medium_1000_10_20_5_3_1-0.5-1.json.gz     TRACE2VEC  0.346919  0.492818   \n",
       "0  medium_1000_10_20_5_3_1-0.5-1.json.gz    CASE2VEC E  0.474781  0.562492   \n",
       "0  medium_1000_10_20_5_3_1-0.5-1.json.gz  CASE2VEC E+C  -0.03014  0.141137   \n",
       "0  medium_1000_10_20_5_3_1-0.5-1.json.gz           GRU  0.846497   0.90816   \n",
       "0  medium_1000_10_20_5_3_1-0.5-1.json.gz          LSTM   0.99549  0.990268   \n",
       "0      p2p_500_10_20_5_1_1-0.8-1.json.gz   Autoencoder  0.258863  0.366568   \n",
       "0      p2p_500_10_20_5_1_1-0.8-1.json.gz     TRACE2VEC  0.244757  0.347538   \n",
       "0      p2p_500_10_20_5_1_1-0.8-1.json.gz    CASE2VEC E   0.04789  0.109349   \n",
       "0      p2p_500_10_20_5_1_1-0.8-1.json.gz  CASE2VEC E+C -0.004501  0.116917   \n",
       "0      p2p_500_10_20_5_1_1-0.8-1.json.gz           GRU  0.982913   0.97299   \n",
       "0      p2p_500_10_20_5_1_1-0.8-1.json.gz          LSTM  0.967043   0.95589   \n",
       "0   small_5000_10_20_5_1_1-0.5-1.json.gz   Autoencoder  0.597131  0.594571   \n",
       "0   small_5000_10_20_5_1_1-0.5-1.json.gz     TRACE2VEC  0.896071  0.877863   \n",
       "0   small_5000_10_20_5_1_1-0.5-1.json.gz    CASE2VEC E  0.645112  0.799717   \n",
       "0   small_5000_10_20_5_1_1-0.5-1.json.gz  CASE2VEC E+C  0.045916  0.201651   \n",
       "0   small_5000_10_20_5_1_1-0.5-1.json.gz           GRU  0.893706  0.895669   \n",
       "0   small_5000_10_20_5_1_1-0.5-1.json.gz          LSTM  0.996764  0.991488   \n",
       "0   wide_10000_10_20_5_3_1-0.3-1.json.gz   Autoencoder  0.178657  0.244695   \n",
       "0   wide_10000_10_20_5_3_1-0.3-1.json.gz     TRACE2VEC  0.260238  0.391607   \n",
       "0   wide_10000_10_20_5_3_1-0.3-1.json.gz    CASE2VEC E  0.227857  0.391211   \n",
       "0   wide_10000_10_20_5_3_1-0.3-1.json.gz  CASE2VEC E+C  0.068301    0.1024   \n",
       "0   wide_10000_10_20_5_3_1-0.3-1.json.gz           GRU  0.995614  0.990799   \n",
       "0   wide_10000_10_20_5_3_1-0.3-1.json.gz          LSTM   0.99547  0.990728   \n",
       "0    wide_5000_10_20_5_1_1-1.0-1.json.gz   Autoencoder  0.080231  0.122571   \n",
       "0    wide_5000_10_20_5_1_1-1.0-1.json.gz     TRACE2VEC  0.289056  0.441972   \n",
       "0    wide_5000_10_20_5_1_1-1.0-1.json.gz    CASE2VEC E   0.28533  0.435407   \n",
       "0    wide_5000_10_20_5_1_1-1.0-1.json.gz  CASE2VEC E+C  -0.00071   0.00449   \n",
       "0    wide_5000_10_20_5_1_1-1.0-1.json.gz           GRU     0.986  0.978016   \n",
       "0    wide_5000_10_20_5_1_1-1.0-1.json.gz          LSTM   0.98595  0.978546   \n",
       "\n",
       "         b3 b4        b5        b6  \\\n",
       "0  0.555623  0   0.38651  0.523406   \n",
       "0  0.584802  0  0.561198  0.556091   \n",
       "0   0.54788  0  0.496726  0.501469   \n",
       "0  0.523436  0  0.282293  0.396285   \n",
       "0  0.996812  0  0.992822  0.992762   \n",
       "0   0.99101  0  0.985109  0.984184   \n",
       "0  0.532924  0  0.416458  0.328539   \n",
       "0   0.59945  0  0.626474  0.481537   \n",
       "0   0.70831  0  0.726941  0.582444   \n",
       "0  0.550673  0   0.15645  0.263381   \n",
       "0  0.992232  0  0.992645  0.973158   \n",
       "0  0.994112  0  0.991421  0.981963   \n",
       "0  0.519418  0  0.382945  0.317043   \n",
       "0  0.575945  0   0.56918  0.434522   \n",
       "0  0.646334  0  0.637994  0.502969   \n",
       "0   0.51745  0  0.116432  0.179148   \n",
       "0  0.916555  0  0.995718  0.834756   \n",
       "0  0.996038  0  0.991043  0.989494   \n",
       "0  0.500469  0  0.387241   0.34799   \n",
       "0  0.490639  0  0.362317  0.333917   \n",
       "0  0.308063  0  0.118354  0.101618   \n",
       "0  0.475764  0  0.078429  0.229584   \n",
       "0  0.984832  0   0.96508   0.98103   \n",
       "0  0.974275  0  0.965305  0.946658   \n",
       "0   0.71133  0  0.713602  0.509573   \n",
       "0  0.905032  0  0.996913  0.784213   \n",
       "0  0.772311  0  0.995913    0.6681   \n",
       "0  0.443248  0  0.226905  0.181455   \n",
       "0  0.918404  0       1.0  0.811051   \n",
       "0  0.997013  0       1.0  0.983119   \n",
       "0  0.375316  0  0.257948  0.232738   \n",
       "0  0.478975  0  0.402698   0.38111   \n",
       "0  0.474234  0  0.397109  0.385484   \n",
       "0  0.299916  0  0.106968  0.098207   \n",
       "0   0.99641  0  0.990741  0.990856   \n",
       "0  0.996411  0  0.990646   0.99081   \n",
       "0  0.325481  0  0.118206  0.127271   \n",
       "0  0.504116  0  0.435377   0.44877   \n",
       "0  0.501381  0  0.428274  0.442781   \n",
       "0  0.377707  0  0.002366  0.043861   \n",
       "0  0.989293  0   0.97803  0.978002   \n",
       "0  0.989297  0  0.978413  0.978678   \n",
       "\n",
       "                                              b7  \n",
       "0       {1: 2444, 2: 88, 3: 878, 4: 1542, 5: 48}  \n",
       "0     {1: 1108, 2: 544, 3: 853, 4: 895, 5: 1600}  \n",
       "0    {1: 1108, 2: 354, 3: 1043, 4: 1600, 5: 895}  \n",
       "0      {1: 392, 2: 372, 3: 526, 4: 374, 5: 3336}  \n",
       "0      {1: 1788, 2: 612, 3: 807, 4: 901, 5: 892}  \n",
       "0      {1: 1788, 2: 897, 3: 880, 4: 803, 5: 632}  \n",
       "0        {1: 240, 2: 295, 3: 129, 4: 63, 5: 273}  \n",
       "0        {1: 309, 2: 220, 3: 222, 4: 83, 5: 166}  \n",
       "0        {1: 158, 2: 78, 3: 146, 4: 222, 5: 396}  \n",
       "0           {1: 24, 2: 50, 3: 60, 4: 42, 5: 824}  \n",
       "0          {1: 427, 2: 6, 3: 366, 4: 56, 5: 145}  \n",
       "0          {1: 432, 2: 365, 3: 59, 4: 141, 5: 3}  \n",
       "0         {1: 239, 2: 335, 3: 78, 4: 54, 5: 294}  \n",
       "0       {1: 305, 2: 206, 3: 244, 4: 133, 5: 112}  \n",
       "0        {1: 165, 2: 80, 3: 140, 4: 340, 5: 275}  \n",
       "0           {1: 75, 2: 37, 3: 801, 4: 43, 5: 44}  \n",
       "0         {1: 333, 2: 99, 3: 366, 4: 144, 5: 58}  \n",
       "0          {1: 432, 2: 364, 3: 146, 4: 57, 5: 1}  \n",
       "0         {1: 77, 2: 112, 3: 153, 4: 127, 5: 31}  \n",
       "0          {1: 57, 2: 188, 3: 82, 4: 41, 5: 132}  \n",
       "0           {1: 153, 2: 82, 3: 83, 4: 85, 5: 97}  \n",
       "0           {1: 14, 2: 11, 3: 450, 4: 10, 5: 15}  \n",
       "0          {1: 100, 2: 72, 3: 196, 4: 131, 5: 1}  \n",
       "0          {1: 71, 2: 101, 3: 133, 4: 188, 5: 7}  \n",
       "0     {1: 955, 2: 239, 3: 798, 4: 1737, 5: 1271}  \n",
       "0     {1: 1230, 2: 2423, 3: 606, 4: 294, 5: 447}  \n",
       "0    {1: 1230, 2: 616, 3: 730, 4: 1407, 5: 1017}  \n",
       "0      {1: 2685, 2: 548, 3: 790, 4: 336, 5: 641}  \n",
       "0      {1: 89, 2: 2332, 3: 1230, 4: 426, 5: 923}  \n",
       "0       {1: 2421, 2: 4, 3: 1345, 4: 11, 5: 1219}  \n",
       "0  {1: 1137, 2: 2054, 3: 2390, 4: 3114, 5: 1305}  \n",
       "0  {1: 2806, 2: 3726, 3: 1173, 4: 1030, 5: 1265}  \n",
       "0  {1: 1354, 2: 1050, 3: 1066, 4: 2102, 5: 4428}  \n",
       "0  {1: 2180, 2: 3583, 3: 1289, 4: 1065, 5: 1883}  \n",
       "0    {1: 3050, 2: 2659, 3: 2252, 4: 36, 5: 2003}  \n",
       "0    {1: 2004, 2: 36, 3: 2247, 4: 2662, 5: 3051}  \n",
       "0     {1: 629, 2: 1963, 3: 627, 4: 267, 5: 1514}  \n",
       "0     {1: 567, 2: 554, 3: 604, 4: 1382, 5: 1893}  \n",
       "0     {1: 724, 2: 532, 3: 472, 4: 1390, 5: 1882}  \n",
       "0           {1: 30, 2: 4938, 3: 10, 4: 5, 5: 17}  \n",
       "0     {1: 1403, 2: 866, 3: 989, 4: 320, 5: 1422}  \n",
       "0     {1: 1403, 2: 1422, 3: 866, 4: 991, 5: 318}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7d6d89260849ad6fae2ac6ef1f683d55b51012ca6af9675811128abf3205473"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('replearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
